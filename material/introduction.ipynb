{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bdf3859",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb221d9",
   "metadata": {},
   "source": [
    "As described in the introductory slides, GPU programming requires strategies for data handling and parallel computation on the device.\n",
    "Below is a summary of the key takeaways and connection points to the next notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a1d864",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data Handling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe67538",
   "metadata": {},
   "source": [
    "Data organization on heterogeneous systems requires:\n",
    "* Allocating and deallocating memory on the host and device (explicit memory, EM), _or_\n",
    "* Allocating and deallocating memory in a unified virtual address space (managed memory, MM),\n",
    "as well as for\n",
    "* Copying data (EM) _or_ migrating data (MM) between host and device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b87a916",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Parallel Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0279bd8",
   "metadata": {},
   "source": [
    "Parallel computation on GPUs requires:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a2131",
   "metadata": {},
   "source": [
    "**1. Trigger execution on GPUs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286cabd",
   "metadata": {},
   "source": [
    "**2. Spawn threads**\n",
    "\n",
    "GPUs, like other hardware components, are designed with a hierarchical structure.\n",
    "To efficiently utilize the hardware, threads and their organization are also typically hierarchical.\n",
    "\n",
    "* CUDA/HIP: *thread* > *block* > *grid*\n",
    "* SYCL: *work item* > *workgroup* > *nd-range*\n",
    "* OpenMP: *thread* > *team* > *league*\n",
    "* OpenACC: *thread* > *vector* > *worker* > *gang*\n",
    "* Kokkos: *thread* > *team* > *league*\n",
    "\n",
    "On the hardware level, threads are further grouped as follows:\n",
    "* *Warps* of 32 on NVIDIA GPUs\n",
    "* *Wavefronts* of 64 on AMD GPUs\n",
    "* *Sub-groups* or *sub-workgroups* on Intel GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a3d0f6",
   "metadata": {},
   "source": [
    "**3. Map threads**\n",
    "\n",
    "Each thread executes the same set of operations.\n",
    "To differentiate them, each thread is assigned one or more IDs or indices, which are used to calculate *globally unique thread indices*.\n",
    "These indices are then used to map threads to specific portions of the work.\n",
    "\n",
    "CUDA/HIP make this explicit by providing *built-in thread variables* that yield different values depending on the evaluating thread.\n",
    "A global thread index is commonly computed from the block index, the block-local thread index, and the block size (number of threads per block) as follows:\n",
    "```cpp\n",
    "blockIdx.x * blockDim.x + threadIdx.x\n",
    "```\n",
    "\n",
    "SYCL and Kokkos provide a global index as a single lambda parameter.\n",
    "\n",
    "OpenMP and OpenACC internally map existing loop indices onto threads.\n",
    "\n",
    "Many standard algorithms do not expose indices directly, but instead operate on references to elements of the input/output data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d6777",
   "metadata": {},
   "source": [
    "**4. Synchronization**\n",
    "\n",
    "Waiting for the GPU to finish outstanding work can be done either:\n",
    "* implicitly at the end of GPU code sections (OpenMP, OpenACC), or\n",
    "* via specific API function calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3ccae4",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9298c",
   "metadata": {},
   "source": [
    "Generally, there are three main approaches to implementing parallel computations on GPUs:\n",
    "* Writing a dedicated GPU kernel (function) as a separate code section, launched from the host code\n",
    "* Defining an inline kernel for better language integration, while still exposing a GPU-specific implementation\n",
    "* Relying on automatic conversion of code originally written for CPU execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81227f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d83856",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2cebd",
   "metadata": {},
   "source": [
    "Our first example code is (almost) a simple hello world application.\n",
    "Instead of simply printing a single pre-defined message, it prints a range of numbers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252c31a",
   "metadata": {},
   "source": [
    "```cpp\n",
    "for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "    printf(\"%ld\\n\", i0);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d4d8a9",
   "metadata": {},
   "source": [
    "The full example is available in [print-numbers-base.cpp](../src/print-numbers/print-numbers-base.cpp), and can be compiled and executed using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfbf56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -O3 -march=native -std=c++17 -o ../build/print-numbers/print-numbers-base ../src/print-numbers/print-numbers-base.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/print-numbers/print-numbers-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6321aab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Vector Increase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2d1b9",
   "metadata": {},
   "source": [
    "The hello world example does not require handling memory.\n",
    "Our next test case - increasing all elements of an array by one - is more complex in this respect.\n",
    "\n",
    "[increase-base.cpp](../src/increase/increase-base.cpp) shows a serial CPU-only implementation.\n",
    "Its key part is the increase function.\n",
    "\n",
    "```cpp\n",
    "void increase(double* data, size_t nx) {\n",
    "    for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "        data[i0] += 1;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae423aeb",
   "metadata": {},
   "source": [
    "Other tasks performed by our application include:\n",
    "* Parsing command line arguments:\n",
    "    * `nx`: the number of elements in the vector to be processed\n",
    "    * `nItWarmUp`: the number of warm-up iterations\n",
    "    * `nIt`: the number of timed iterations\n",
    "* Allocating an array with `nx` elements\n",
    "* Initializing the array so that each element holds a value equal to its index\n",
    "* Calling `increase` for `nItWarmUp` iterations\n",
    "* Calling `increase` for `nIt` iterations and measuring the time taken\n",
    "* Printing statistics and estimated performance metrics\n",
    "* Verifying that all array elements have the expected value\n",
    "* Deallocating the array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d9f252",
   "metadata": {},
   "source": [
    "You can compile and execute the code using the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -O3 -march=native -std=c++17 -o ../build/increase/increase-base ../src/increase/increase-base.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-base"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
