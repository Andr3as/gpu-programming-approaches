{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac6833bd-6722-4ae6-87e7-4dbbcac9f1e0",
   "metadata": {},
   "source": [
    "# Parallel Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae9fbdd-4f59-4387-a9d9-b02be2576611",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e4aede-376d-44bb-9dab-dc6dfbf8c566",
   "metadata": {},
   "source": [
    "As discussed in the introduction, parallel computation on a GPU involves several key steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1195eec-6909-4e01-881e-1a4c3f00fa73",
   "metadata": {},
   "source": [
    "**1. Trigger execution on GPUs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ee124-9354-4fd2-b954-04a3d893fd38",
   "metadata": {},
   "source": [
    "**2. Spawn threads**\n",
    "\n",
    "GPUs, like other hardware components, are designed with a hierarchical structure.\n",
    "To efficiently utilize the hardware, threads and their organization are also typically hierarchical.\n",
    "\n",
    "* CUDA/HIP: *thread* > *block* > *grid*\n",
    "* SYCL: *work item* > *workgroup* > *nd-range*\n",
    "* OpenMP: *thread* > *team* > *league*\n",
    "* OpenACC: *thread* > *vector* > *worker* > *gang*\n",
    "* Kokkos: *thread* > *team* > *league*\n",
    "\n",
    "On the hardware level, threads are further grouped as follows:\n",
    "* *Warps* of 32 on NVIDIA GPUs\n",
    "* *Wavefronts* of 64 on AMD GPUs\n",
    "* *Sub-groups* or *sub-workgroups* on Intel GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c71437b-bc4a-4b86-90ac-84094d033c18",
   "metadata": {},
   "source": [
    "**3. Map threads**\n",
    "\n",
    "Each thread executes the same set of operations.\n",
    "To differentiate them, each thread is assigned one or more IDs or indices, which are used to calculate *globally unique thread indices*.\n",
    "These indices are then used to map threads to specific portions of the work.\n",
    "\n",
    "CUDA/HIP make this explicit by providing *built-in thread variables* that yield different values depending on the evaluating thread.\n",
    "A global thread index is commonly computed from the block index, the block-local thread index, and the block size (number of threads per block) as follows:\n",
    "```cpp\n",
    "blockIdx.x * blockDim.x + threadIdx.x\n",
    "```\n",
    "\n",
    "SYCL and Kokkos provide a global index as a single lambda parameter.\n",
    "\n",
    "OpenMP and OpenACC internally map existing loop indices onto threads.\n",
    "\n",
    "Many standard algorithms do not expose indices directly, but instead operate on references to elements of the input/output data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f736a-4001-43d7-a400-af6dd0e98d36",
   "metadata": {},
   "source": [
    "**4. Synchronization**\n",
    "\n",
    "Waiting for the GPU to finish outstanding work can be done either:\n",
    "* implicitly at the end of GPU code sections (OpenMP, OpenACC), or\n",
    "* via specific API function calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12cbefa-c2ca-4fac-9873-e20ef93d40a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d43f4d8-d81f-4d4d-8f7d-21487ab53821",
   "metadata": {},
   "source": [
    "Generally, there are three main approaches to implementing parallel computations on GPUs:\n",
    "* Writing a dedicated GPU kernel (function) as a separate code section, launched from the host code\n",
    "* Defining an inline kernel for better language integration, while still exposing a GPU-specific implementation\n",
    "* Relying on automatic conversion of code originally written for CPU execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd0bd8-4dde-4180-89da-2ce6357a3226",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aa482d-cffe-4fed-b4f7-34eb7b5849d1",
   "metadata": {},
   "source": [
    "To demonstrate the different offloading and parallelization approaches, we consider a simple test case: increasing all elements of an array by one. \\\n",
    "[increase-base.cpp](../src/increase/increase-base.cpp) shows a serial CPU-only implementation.\n",
    "Its key part is the increase function.\n",
    "\n",
    "```cpp\n",
    "void increase(double* data, size_t nx) {\n",
    "    for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "        data[i0] += 1;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d570ea5-896d-442b-af85-75935596ee5b",
   "metadata": {},
   "source": [
    "Other tasks performed by our application include:\n",
    "* Parsing command line arguments:\n",
    "    * `nx`: the number of elements in the vector to be processed\n",
    "    * `nItWarmUp`: the number of warm-up iterations\n",
    "    * `nIt`: the number of timed iterations\n",
    "* Allocating an array with `nx` elements\n",
    "* Initializing the array so that each element holds a value equal to its index\n",
    "* Calling `increase` for `nItWarmUp` iterations\n",
    "* Calling `increase` for `nIt` iterations and measuring the time taken\n",
    "* Printing statistics and estimated performance metrics\n",
    "* Verifying that all array elements have the expected value\n",
    "* Deallocating the array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd874f7-0279-40d4-926d-8566c6eca418",
   "metadata": {},
   "source": [
    "You can compile and execute the code using the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2090492e-ed48-4316-a599-86f8412d0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -O3 -march=native -std=c++17 -o ../build/increase/increase-base ../src/increase/increase-base.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc3169-bfa4-4451-b7d2-49a36a38f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f2daa5-4900-46d6-a6c8-8bf871bc3618",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## OpenMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906d761e-8bbe-43d6-bdf8-6a548b2d7e70",
   "metadata": {},
   "source": [
    "**1.** OpenMP enables code execution on GPUs by introducing *target regions*.\n",
    "\n",
    "```cpp\n",
    "#pragma omp target\n",
    "for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "    data[i0] += 1;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f349fd4-8f51-4f38-ae34-1bffa7655bed",
   "metadata": {},
   "source": [
    "**2.** This code executes the loop on the GPU *serially*. To introduce parallelism, use `teams` and `parallel`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f312d-335f-46a4-8bcf-7acad1bfd9b2",
   "metadata": {},
   "source": [
    "**3.** Loop iterations can be mapped to spawned threads using `distribute` and `for`.\n",
    "If not specified, the compiler chooses the number of teams and threads per team.\n",
    "\n",
    "```cpp\n",
    "#pragma omp target teams distribute parallel for\n",
    "for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "    data[i0] += 1;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45808a10-1fd5-4ba2-87fa-afd8996e2901",
   "metadata": {},
   "source": [
    "**4.** Synchronization occurs implicitly at the end of the target region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777bfddd-bd6f-4695-b21c-9f131c73f2db",
   "metadata": {},
   "source": [
    "The complete example code can be found in [increase-omp-target-expl.cpp](../src/increase/increase-omp-target-expl.cpp) and [increase-omp-target-mm.cpp](../src/increase/increase-omp-target-mm.cpp).\n",
    "Build and execute them using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac0144b-5525-47b9-bcb6-86ac4166f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -mp=gpu -target=gpu -o ../build/increase/increase-omp-target-expl ../src/increase/increase-omp-target-expl.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e0b45d-29ef-4d88-a97d-7962c7bd8d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-omp-target-expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d7353-7628-4d43-8a58-3366f1677706",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -mp=gpu -target=gpu -gpu=mem:managed -o ../build/increase/increase-omp-target-mm ../src/increase/increase-omp-target-mm.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b0cb72-48d3-485a-bbc5-62a8d8ab8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-omp-target-mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf6332-5d77-44cc-8557-d240c5af0210",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## OpenACC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc077ab-9582-4e14-96f2-c596aab5b8f7",
   "metadata": {},
   "source": [
    "OpenACC offers a similar approach to OpenMP target offloading, using `parallel` (to spawn threads) and `loop` (to distribute work).\n",
    "\n",
    "Whether execution occurs on the CPU or GPU is determined by compiler arguments.\n",
    "\n",
    "If not specified, the compiler chooses the number of gangs, workers, and vector size.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop\n",
    "for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "    data[i0] += 1;\n",
    "}   // implicit synchronization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c6b3f-b987-4ff7-a5b9-0db97fadb1c7",
   "metadata": {},
   "source": [
    "As with OpenMP, this code instructs the compiler to parallelize the loop - regardless of whether it is safe (e.g., in the presence of race conditions due to inter-iteration dependencies).\n",
    "Alternatively, `kernels` can be used to give the compiler more control.\n",
    "In this case, the compiler will:\n",
    "* Analyze dependencies and only parallelize loops without dependencies\n",
    "* Apply loop and kernel transformations, including fusion\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels\n",
    "{\n",
    "    for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "        data[i0] += 1;\n",
    "    }\n",
    "    /* potentially more work */\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895cd733-c349-436b-ae0f-ae592fd8d55b",
   "metadata": {},
   "source": [
    "The complete example code is available in [increase-openacc-expl.cpp](../src/increase/increase-openacc-expl.cpp) and [increase-openacc-mm.cpp](../src/increase/increase-openacc-mm.cpp).\n",
    "Build and execute them using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d011c-695a-403b-bdcf-5a0d99465701",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -acc=gpu -target=gpu -o ../build/increase/increase-openacc-expl ../src/increase/increase-openacc-expl.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d68e8-1d06-4e68-bbf1-551822325c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-openacc-expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a599ec-6fb4-4ff4-a24a-491c6e2ce175",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -acc=gpu -target=gpu -gpu=mem:managed -o ../build/increase/increase-openacc-mm ../src/increase/increase-openacc-mm.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7fd88-7e67-4a1f-88a0-98b1e79a9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-openacc-mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9086bd-6847-47ef-8797-00ee3bdd0ec0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Modern C++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99683c5d-7d92-404e-bcce-bb3faa4ad89b",
   "metadata": {},
   "source": [
    "An alternative to loop-based operations is to use STL algorithms.\n",
    "These can be parallelized by providing an *execution policy*, and GPU offloading can be enabled via compiler arguments.\n",
    "\n",
    "```cpp\n",
    "std::transform(std::execution::par_unseq, data, data + nx, data,\n",
    "               [=](auto data_item) {\n",
    "                   return data_item + 1;\n",
    "               }); // implicit synchronization\n",
    "```\n",
    "\n",
    "All algorithm executions are synchronous with respect to the CPU, e.g. no explicit GPU synchronization is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d201454-eb9b-4f4a-92ae-4697412f29c3",
   "metadata": {},
   "source": [
    "If indices are needed (for example, to access neighboring elements), there are two main approaches.\n",
    "\n",
    "Reconstruct the index using pointer arithmetic...\n",
    "```cpp\n",
    "std::for_each(std::execution::par_unseq, data, data + nx,\n",
    "              [=](const auto& data_item) {\n",
    "                  const size_t i0 = &data_item - data;\n",
    "                  data[i0] += 1;\n",
    "              });\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60bfd75-e1fb-4554-912c-c747e7e5a958",
   "metadata": {},
   "source": [
    "...or use a thrust `counting_iterator` (also available on AMD via *rocThrust*):\n",
    "```cpp\n",
    "std::for_each(std::execution::par_unseq, thrust::make_counting_iterator<size_t>(0), thrust::make_counting_iterator<size_t>(nx),\n",
    "              [=](const auto &i0) {\n",
    "                  data[i0] += 1;\n",
    "              });\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437bcb9f-0552-4281-85af-74fffed7f04e",
   "metadata": {},
   "source": [
    "The complete example code is available in [increase-std-par.cpp](../src/increase/increase-std-par.cpp).\n",
    "Build and execute it using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8148286-e645-4f66-94d9-6ee8fdd1badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -stdpar=gpu -target=gpu -gpu=cc86 -o ../build/increase/increase-std-par ../src/increase/increase-std-par.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba88e8c6-1c54-4909-9c9f-f600fd03e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-std-par"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e98bbf9-35d1-4cd8-bcdd-0fc75b37183a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Thrust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43835fa1-966a-4c1d-8533-753138e588e4",
   "metadata": {},
   "source": [
    "For more control and support for additional computational patterns, Thrust is a strong alternative over 'standard' modern C++.\n",
    "\n",
    "It provides GPU-accelerated versions of many STL algorithms, as well as additional ones.\n",
    "Thrust algorithms also accept an *execution policy* argument, which specifies *where* the computation should be performed (note the difference to `std::execution`).\n",
    "\n",
    "```cpp\n",
    "thrust::transform(thrust::device, data.begin(), data.end(), data.begin(),\n",
    "                  [=] __host__ __device__ (double data_elem) {\n",
    "                      return data_elem + 1;\n",
    "                  }); // implicit synchronization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc92dd-79f5-4d5f-8344-783a52ed7f6a",
   "metadata": {},
   "source": [
    "As before, you can also use a counting iterator.\n",
    "\n",
    "```cpp\n",
    "double *data_ptr = thrust::raw_pointer_cast(data.data());\n",
    "thrust::for_each(thrust::device, thrust::make_counting_iterator<size_t>(0), thrust::make_counting_iterator<size_t>(nx),\n",
    "                 [=] __host__ __device__ (size_t i0) {\n",
    "                     data_ptr[i0] += 1;\n",
    "                 });\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df4915-99dc-40e0-8399-601e6d54a04e",
   "metadata": {},
   "source": [
    "Alternatively, Thrust provides the `tabulate` pattern.\n",
    "It applies a transformation to the *index* of each element and stores the result in-place.\n",
    "\n",
    "```cpp\n",
    "double *data_ptr = thrust::raw_pointer_cast(data.data());\n",
    "thrust::tabulate(thrust::device, data.begin(), data.end(),\n",
    "                 [=] __host__ __device__ (size_t i0) {\n",
    "                     return data_ptr[i0] + 1;\n",
    "                 });\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1fc2ea",
   "metadata": {},
   "source": [
    "As with STL algorithms, thrust counterparts are synchronous with respect to the CPU, e.g. no explicit GPU synchronization is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae00fcd8-a494-4137-bf3e-57f74686628c",
   "metadata": {},
   "source": [
    "The complete example code is available in [increase-thrust-expl.cu](../src/increase/increase-thrust-expl.cu) and [increase-thrust-mm.cu](../src/increase/increase-thrust-mm.cu).\n",
    "Build and execute them using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d99cf5-f9a1-402d-a343-1254188f5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -std=c++17 --extended-lambda -arch=sm_86 -o ../build/increase/increase-thrust-expl ../src/increase/increase-thrust-expl.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a5203d-1456-494b-94f6-e691bce37d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-thrust-expl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e0cca-7136-4578-bd19-98f5eb5e0320",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -std=c++17 --extended-lambda -arch=sm_86 -o ../build/increase/increase-thrust-mm ../src/increase/increase-thrust-mm.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6d94f-6b8e-4ba8-be37-560efd51baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-thrust-mm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917661a6-d702-45ec-9ca1-f8f1e72e27d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Kokkos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7ebfd-74b4-4e2f-a6ca-bf71232e75ba",
   "metadata": {},
   "source": [
    "Kokkos provides its own abstraction for parallel loops.\n",
    "Depending on how Kokkos is compiled, this will map to either CPU or GPU execution spaces.\n",
    "\n",
    "```cpp\n",
    "Kokkos::parallel_for(\n",
    "    Kokkos::RangePolicy<>(0, nx),\n",
    "        KOKKOS_LAMBDA(const size_t i0) {\n",
    "            data(i0) += 1;\n",
    "        });\n",
    "```\n",
    "\n",
    "Tuning the thread hierarchy can be done by specifying an additional *team policy*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7b22a2-4be2-4f7d-b045-18154e53a7ad",
   "metadata": {},
   "source": [
    "Synchronization with the GPU is *not* implicit.\n",
    "It can be triggered by calling:\n",
    "```cpp\n",
    "Kokkos::fence();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d586b971-0975-4aa1-ade8-f6069ee80df7",
   "metadata": {},
   "source": [
    "The complete example code is available in [increase-kokkos.cpp](../src/increase/increase-kokkos.cpp).\n",
    "Build and execute it using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2cd8d8-7f4b-4165-8fe3-7889fa22f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -O3 -march=native -std=c++20 -I/root/kokkos/install-serial/include -L/root/kokkos/install-serial/lib -o ../build/increase/increase-kokkos-serial ../src/increase/increase-kokkos.cpp -lkokkoscore -ldl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e51e0f-40da-4fb2-bc7a-24f514c6f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-kokkos-serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c712b811-0ef3-4191-afe3-61f0fc4fc696",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/root/kokkos/install-cuda/bin/nvcc_wrapper -O3 -march=native -std=c++20 -arch=sm_86 --expt-extended-lambda --expt-relaxed-constexpr -I/root/kokkos/install-cuda/include -L/root/kokkos/install-cuda/lib -o ../build/increase/increase-kokkos-cuda ../src/increase/increase-kokkos.cpp -lkokkoscore -ldl -lcuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af276d-9470-45eb-adf2-8772757ff5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-kokkos-cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90718e19-55c2-4b5c-9f54-1e168ef94dc0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SYCL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bcbd81-aa22-459f-834f-5e1b6f76dc68",
   "metadata": {},
   "source": [
    "SYCL also provides an abstraction for parallel loops.\n",
    "Using it requires a *handler*, which in turn requires a work *queue*.\n",
    "\n",
    "The latter can be initialized with the `in_order` property that ensures that kernels and other operations are executed in order.\n",
    "\n",
    "```cpp\n",
    "sycl::queue q(sycl::property::queue::in_order{});\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc4f2e",
   "metadata": {},
   "source": [
    "Work can then be submitted to the queue which provides a handler\n",
    "\n",
    "```cpp\n",
    "q.submit([&](sycl::handler &h) {\n",
    "    h.parallel_for(nx, [=](auto i0) {\n",
    "        data[i0] += 1;\n",
    "    });\n",
    "});\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5b95e-9a38-435b-9cda-a22ca0d68b77",
   "metadata": {},
   "source": [
    "You can tune the workgroup size by specifying global and local sizes (the total number of threads and the number of threads per workgroup).\n",
    "Note that these must be evenly divisible, and any extra threads may need to be masked.\n",
    "\n",
    "```cpp\n",
    "auto local_size = 256;\n",
    "auto global_size = ceilingDivide(nx, local_size) * local_size;\n",
    "q.submit([&](sycl::handler &h) {\n",
    "    h.parallel_for( sycl::nd_range<1>{ global_size, local_size }, [=](auto item) {\n",
    "        auto i0 = item.get_global_id(0);\n",
    "        if (i0 < nx) {\n",
    "            data[i0] += 1;\n",
    "        }\n",
    "    });\n",
    "});\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5695c290-f8e7-44e2-b999-d0524780faca",
   "metadata": {},
   "source": [
    "In all cases, explicit synchronization with the GPU is performed by calling:\n",
    "\n",
    "```cpp\n",
    "q.wait();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cabc914-9445-4dbf-be4b-2f4df0674048",
   "metadata": {},
   "source": [
    "When using buffers, you must create additional accessors to access data.\n",
    "\n",
    "```cpp\n",
    "q.submit([&](sycl::handler &h) {\n",
    "    auto data = b_data.get_access(h, sycl::read_write);\n",
    "    h.parallel_for(nx, [=](auto i0) {\n",
    "        data[i0] += 1;\n",
    "    });\n",
    "});\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c16d8-82a3-4da8-ba90-c12ac5cec41d",
   "metadata": {},
   "source": [
    "The complete example code is available in [increase-sycl-expl.cpp](../src/increase/increase-sycl-expl.cpp), [increase-sycl-mm.cpp](../src/increase/increase-sycl-mm.cpp), and [increase-sycl-buffer.cpp](../src/increase/increase-sycl-buffer.cpp).\n",
    "Build and execute them using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2730b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!icpx -O3 -march=native -std=c++17 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_86 -o ../build/increase/increase-sycl-expl ../src/increase/increase-sycl-expl.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf309a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-sycl-expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8619ce12-b023-40d4-9db9-a7a0c23c864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!icpx -O3 -march=native -std=c++17 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_86 -o ../build/increase/increase-sycl-buffer ../src/increase/increase-sycl-buffer.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fc1c8-b18d-44e1-a853-e397e1f2fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-sycl-buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc0cb03-4af5-4161-87ea-7c6e4cf66007",
   "metadata": {},
   "outputs": [],
   "source": [
    "!icpx -O3 -march=native -std=c++17 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_86 -o ../build/increase/increase-sycl-expl ../src/increase/increase-sycl-expl.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c7b26-d518-4702-b15c-29d17a881d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-sycl-expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e590b96-5105-46a0-8f19-1a3d130d567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!icpx -O3 -march=native -std=c++17 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_86 -o ../build/increase/increase-sycl-mm ../src/increase/increase-sycl-mm.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675b009-cf18-47a3-9d64-01726e92be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-sycl-mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee23cf3-581b-4234-8e37-090cb15e08a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CUDA/ HIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2655e2-3628-4ea8-9e41-2369d36378ba",
   "metadata": {},
   "source": [
    "CUDA/HIP utilize separate *kernel* functions which are *launched* from the host code. By convention, they must return `void` and are marked with the `__global__` keyword.\n",
    "\n",
    "```cpp\n",
    "__global__ void increase(double* data, size_t nx) {\n",
    "    for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "        data[i0] += 1;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The kernel can be configured by providing an *execution configuration* in triple-chevron syntax.\n",
    "\n",
    "```cpp\n",
    "increase<<<1, 1>>>(d_data, nx);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae0f4d-26cb-4669-96bf-c74840c4a845",
   "metadata": {},
   "source": [
    "The above example runs on the GPU, but all work is done in a single thread.\n",
    "To achieve parallelism, you must manually assign each loop iteration to a separate thread and spawn as many threads as there are iterations.\n",
    "Each thread computes a unique global or data index using built-in thread variables.\n",
    "\n",
    "```cpp\n",
    "__global__ void increase(double* data, size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    data[i0] += 1;\n",
    "}\n",
    "```\n",
    "\n",
    "```cpp\n",
    "auto numThreadsPerBlock = 256;\n",
    "auto numBlocks = nx / numThreadsPerBlock;\n",
    "increase<<<numBlocks, numThreadsPerBlock>>>(d_data, nx);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2a5df-65f2-40b1-bf50-fd341e8a5182",
   "metadata": {},
   "source": [
    "While the above example works if `nx` is evenly divisible by the block size, it will not in all other cases.\n",
    "The common solution is to spawn an extra block and ensure that only valid threads perform computations.\n",
    "\n",
    "```cpp\n",
    "__global__ void increase(double* data, size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (i0 < nx)\n",
    "        data[i0] += 1;\n",
    "}\n",
    "```\n",
    "\n",
    "```cpp\n",
    "auto numThreadsPerBlock = 256;\n",
    "auto numBlocks = ceilingDivide(nx, numThreadsPerBlock);\n",
    "increase<<<numBlocks, numThreadsPerBlock>>>(d_data, nx);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457bbae-d6f6-4387-b7cd-5a7727badea4",
   "metadata": {},
   "source": [
    "The complete example code is available in [increase-cuda-expl.cpp](../src/increase/increase-cuda-expl.cpp) and [increase-cuda-mm.cpp](../src/increase/increase-cuda-mm.cpp).\n",
    "Build and execute them using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd095be7-a688-4bbb-8514-7f3b0577c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -std=c++17 -arch=sm_86 -o ../build/increase/increase-cuda-expl ../src/increase/increase-cuda-expl.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848961e-7902-4df3-a394-fed49763cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-cuda-expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a9ee2-1aca-4faa-a706-106b542e7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -std=c++17 -arch=sm_86 -o ../build/increase/increase-cuda-mm ../src/increase/increase-cuda-mm.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cd176-99f6-4400-a649-d838d0d17f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-cuda-mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd4ce7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Next Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022cc009",
   "metadata": {},
   "source": [
    "Proceed to the [next steps](./next-steps.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
