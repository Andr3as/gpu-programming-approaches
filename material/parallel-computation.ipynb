{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac6833bd-6722-4ae6-87e7-4dbbcac9f1e0",
   "metadata": {},
   "source": [
    "# Parallel Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ee3fd3-347c-4173-8357-e52b913cc945",
   "metadata": {},
   "source": [
    "As discussed in more detail in the introduction, parallel computation on a GPU requires multiple steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1195eec-6909-4e01-881e-1a4c3f00fa73",
   "metadata": {},
   "source": [
    "**1. Trigger execution on GPUs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ee124-9354-4fd2-b954-04a3d893fd38",
   "metadata": {},
   "source": [
    "**2. Spawn threads**\n",
    "\n",
    "GPUs, similar to other hardware components, are designed to be hierarchical.\n",
    "To efficiently map to the underlying hardware, threads and their organization is also often hierarchical.\n",
    "\n",
    "* CUDA/HIP organizes *thread* > *block* > *grid*\n",
    "* SYCL organizes *work item* > *workgroup* > *nd-range*\n",
    "* OpenMP organizes *thread* > *team* > *league*\n",
    "* OpenACC organizes *thread* > *vector* > *worker* > *gang*\n",
    "* Kokkos organizes *thread* > *team* > *league*\n",
    "\n",
    "Threads are additionally grouped on the hardware level in\n",
    "* *Warps* of size 32 on NVIDIA,\n",
    "* *Wavefronts* of size 64 on AMD, and\n",
    "* *Sub-groups*/ *sub-workgroups* on Intel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c71437b-bc4a-4b86-90ac-84094d033c18",
   "metadata": {},
   "source": [
    "**3. Map threads**\n",
    "\n",
    "Each thread essentially executes the same (series of) operations.\n",
    "To distinguish them, each thread has one or more IDs or indices which can be used to calculate *globally unique thread indices*.\n",
    "These can then used to map threads to the parts of the work to be done.\n",
    "\n",
    "CUDA/HIP make this explicit by provided *build-in thread variables* which, when evaluated, provide different values depending on the evaluating thread.\n",
    "A global thread index is commonly computed from the block index, the block-local thread index and the block size (the number of threads per block) by evaluating\n",
    "```cpp\n",
    "blockIdx.x * blockDim.x + threadIdx.x\n",
    "```\n",
    "\n",
    "SYCL and Kokkos provide a global index as a single lambda parameter.\n",
    "\n",
    "OpenMP and OpenACC internally map already existing loop indices onto threads.\n",
    "\n",
    "Many standard algorithms don't expose indices at all, but instead work on references to elements of the input/ output data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f736a-4001-43d7-a400-af6dd0e98d36",
   "metadata": {},
   "source": [
    "**4. Synchronization**\n",
    "\n",
    "Waiting for the GPU to finish any outstanding work can be done either\n",
    "* implicitly at the end of GPU code parts (OpenMP, OpenACC), or\n",
    "* via specific API function calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12cbefa-c2ca-4fac-9873-e20ef93d40a9",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d43f4d8-d81f-4d4d-8f7d-21487ab53821",
   "metadata": {},
   "source": [
    "Generally speaking, there are three main approaches:\n",
    "* A dedicated GPU kernel (function) implemented as a separate code part and launched from the host code\n",
    "* An inline kernel definition that offers better language integration but still exposes a GPU specific implementation\n",
    "* Automatic conversion of code that was written to run on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd0bd8-4dde-4180-89da-2ce6357a3226",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aa482d-cffe-4fed-b4f7-34eb7b5849d1",
   "metadata": {},
   "source": [
    "To demonstrate the different offloading and parallelization approaches, we consider a simple test case - increasing all elements of an array by one. \\\n",
    "[increase-base.cpp](../src/increase/increase-base.cpp) shows a serial CPU-only implementation.\n",
    "The key part of it is the increase function.\n",
    "\n",
    "```cpp\n",
    "void increase(double* data, size_t nx) {\n",
    "    for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "        data[i0] += 1;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d570ea5-896d-442b-af85-75935596ee5b",
   "metadata": {},
   "source": [
    "Other things our application does are:\n",
    "* parse command line arguments\n",
    "    * `nx`, the number of elements in the vector to be processed\n",
    "    * `nItWarmUp`, the number of warm-up iterations\n",
    "    * `nIt`, the number of timed iterations\n",
    "* allocate an array with `nx` elements\n",
    "* initialize the array such that each element holds a value equal to its index \n",
    "* call `increase` `nItWarmUp` times\n",
    "* call `increase` `nIt` times and take the time required for it\n",
    "* print statistics and estimated performance metrics\n",
    "* check that all array elements have the expected value\n",
    "* deallocate the array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd874f7-0279-40d4-926d-8566c6eca418",
   "metadata": {},
   "source": [
    "The code can be compiled and executed with the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2090492e-ed48-4316-a599-86f8412d0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -O3 -march=native -std=c++17 -o ../build/increase/increase-base ../src/increase/increase-base.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc3169-bfa4-4451-b7d2-49a36a38f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e67d0a1-5e53-49f6-b0c5-c4cdbf261c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -O3 -march=native -std=c++17 -fopenmp -o ../build/increase/increase-omp-host ../src/increase/increase-omp-host.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15698c56-7c4f-4b63-8b25-7a5e143b0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-omp-host"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f2daa5-4900-46d6-a6c8-8bf871bc3618",
   "metadata": {},
   "source": [
    "## OpenMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906d761e-8bbe-43d6-bdf8-6a548b2d7e70",
   "metadata": {},
   "source": [
    "**1.** OpenMP allows code execution on GPUs by introducing *target regions*.\n",
    "\n",
    "```cpp\n",
    "#pragma omp target\n",
    "for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "    data[i0] += 1;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f349fd4-8f51-4f38-ae34-1bffa7655bed",
   "metadata": {},
   "source": [
    "**2.** This code executes the loop on the GPU *serially*.\n",
    "Parallelism can be created with `teams` and `parallel`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f312d-335f-46a4-8bcf-7acad1bfd9b2",
   "metadata": {},
   "source": [
    "**3.** Mapping loop iterations to the spawned threads can be done with `distribute` and `for`.\n",
    "Without further specification, the number of teams and threads per team are chosen by the compiler.\n",
    "\n",
    "```cpp\n",
    "#pragma omp target teams distribute parallel for\n",
    "for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "    data[i0] += 1;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45808a10-1fd5-4ba2-87fa-afd8996e2901",
   "metadata": {},
   "source": [
    "**4.** Synchronization is implicit at the end of the target region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777bfddd-bd6f-4695-b21c-9f131c73f2db",
   "metadata": {},
   "source": [
    "The full example code is available at [increase-omp-target-expl.cpp](../src/increase/increase-omp-target-expl.cpp) and [increase-omp-target-mm.cpp](../src/increase/increase-omp-target-mm.cpp).\n",
    "They can be build and executed using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac0144b-5525-47b9-bcb6-86ac4166f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -mp=gpu -target=gpu -o ../build/increase/increase-omp-target-expl ../src/increase/increase-omp-target-expl.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e0b45d-29ef-4d88-a97d-7962c7bd8d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-omp-target-expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d7353-7628-4d43-8a58-3366f1677706",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -mp=gpu -target=gpu -gpu=mem:managed -o ../build/increase/increase-omp-target-mm ../src/increase/increase-omp-target-mm.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b0cb72-48d3-485a-bbc5-62a8d8ab8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-omp-target-mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf6332-5d77-44cc-8557-d240c5af0210",
   "metadata": {},
   "source": [
    "## OpenACC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc077ab-9582-4e14-96f2-c596aab5b8f7",
   "metadata": {},
   "source": [
    "OpenACC allows a similar approach to the previously discussed OpenMP target offloading by using `parallel` (thread spawning) and `loop` (work distribution).\\\n",
    "Whether execution is ultimately done on CPU or GPU is specified by providing different arguments when compiling.\\\n",
    "Without further specification, the number of gangs and workers, and vector size are chosen by the compiler.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop\n",
    "for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "    data[i0] += 1;\n",
    "}   // implicit synchronization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c6b3f-b987-4ff7-a5b9-0db97fadb1c7",
   "metadata": {},
   "source": [
    "As with OpenMP, this code will trigger the compiler to apply parallelization - whether it is legal or not (e.g. in case of race conditions due to inter-iteration data dependencies).\n",
    "Alternatively, `kernels` may be used to give more control to the compiler.\n",
    "It will then\n",
    "* perform a dependency analysis and only parallelize loops if no loop dependencies were found, and\n",
    "* perform loop/ kernel transformations including fusion.\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels\n",
    "{\n",
    "    for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "        data[i0] += 1;\n",
    "    }\n",
    "    /* potentially more work */\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895cd733-c349-436b-ae0f-ae592fd8d55b",
   "metadata": {},
   "source": [
    "The full example code is available at [increase-openacc-expl.cpp](../src/increase/increase-openacc-expl.cpp) and [increase-openacc-mm.cpp](../src/increase/increase-openacc-mm.cpp).\n",
    "They can be build and executed using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d011c-695a-403b-bdcf-5a0d99465701",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -acc=gpu -target=gpu -o ../build/increase/increase-openacc-expl ../src/increase/increase-openacc-expl.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d68e8-1d06-4e68-bbf1-551822325c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-openacc-expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a599ec-6fb4-4ff4-a24a-491c6e2ce175",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -acc=gpu -target=gpu -gpu=mem:managed -o ../build/increase/increase-openacc-mm ../src/increase/increase-openacc-mm.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7fd88-7e67-4a1f-88a0-98b1e79a9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-openacc-mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9086bd-6847-47ef-8797-00ee3bdd0ec0",
   "metadata": {},
   "source": [
    "## Modern C++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99683c5d-7d92-404e-bcce-bb3faa4ad89b",
   "metadata": {},
   "source": [
    "An alternative to loop-based operations and their parallelization is using STL algorithms.\\\n",
    "They can be marked for parallelization by providing an additional *execution policy*, and GPU offloading can be triggered via compiler arguments.\n",
    "\n",
    "```cpp\n",
    "std::transform(std::execution::par_unseq, data, data + nx, data,\n",
    "               [=](auto data_item) {\n",
    "                   return data_item + 1;\n",
    "               }); // implicit synchronization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d201454-eb9b-4f4a-92ae-4697412f29c3",
   "metadata": {},
   "source": [
    "If indices are required (e.g. for accessing information in some sort of neighborhood), there are two main alternatives.\n",
    "\n",
    "Reconstruction of the index via pointer arithmetic ...\n",
    "```cpp\n",
    "std::for_each(std::execution::par_unseq, data, data + nx,\n",
    "              [=](const auto& data_item) {\n",
    "                  const size_t i0 = &data_item - data;\n",
    "                  data[i0] += 1;\n",
    "              });\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60bfd75-e1fb-4554-912c-c747e7e5a958",
   "metadata": {},
   "source": [
    "... or using a thrust `counting_iterator` (also available on AMD through *rocThrust*)\n",
    "```cpp\n",
    "std::for_each(std::execution::par_unseq, thrust::make_counting_iterator<size_t>(0), thrust::make_counting_iterator<size_t>(nx),\n",
    "              [=](const auto &i0) {\n",
    "                  data[i0] += 1;\n",
    "              });\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437bcb9f-0552-4281-85af-74fffed7f04e",
   "metadata": {},
   "source": [
    "The full example code is available at [increase-std-par.cpp](../src/increase/increase-std-par.cpp).\n",
    "It can be build and executed using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8148286-e645-4f66-94d9-6ee8fdd1badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -stdpar=gpu -target=gpu -gpu=cc86 -o ../build/increase/increase-std-par ../src/increase/increase-std-par.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba88e8c6-1c54-4909-9c9f-f600fd03e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-std-par"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e98bbf9-35d1-4cd8-bcdd-0fc75b37183a",
   "metadata": {},
   "source": [
    "## Thrust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43835fa1-966a-4c1d-8533-753138e588e4",
   "metadata": {},
   "source": [
    "For more control and support for additional computational patterns, thrust can be a viable alternative.\n",
    "It provides GPU-accelerated versions of many STL algorithms as well as additional ones.\n",
    "They also receive an *execution policy* as argument, but in this case it prescribes where to execute the computation.\n",
    "\n",
    "```cpp\n",
    "thrust::transform(thrust::device, data.begin(), data.end(), data.begin(),\n",
    "                  [=] __host__ __device__ (double data_elem) {\n",
    "                      return data_elem + 1;\n",
    "                  }); // implicit synchronization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc92dd-79f5-4d5f-8344-783a52ed7f6a",
   "metadata": {},
   "source": [
    "As before, using a counting iterator is also possible.\n",
    "\n",
    "```cpp\n",
    "double *data_ptr = thrust::raw_pointer_cast(data.data());\n",
    "thrust::for_each(thrust::device, thrust::make_counting_iterator<size_t>(0), thrust::make_counting_iterator<size_t>(nx),\n",
    "                 [=] __host__ __device__ (size_t i0) {\n",
    "                     data_ptr[i0] += 1;\n",
    "                 });\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df4915-99dc-40e0-8399-601e6d54a04e",
   "metadata": {},
   "source": [
    "Alternatively, thrust also provides the `tabulate` pattern which applies a transform on the *index* of each element.\n",
    "\n",
    "```cpp\n",
    "double *data_ptr = thrust::raw_pointer_cast(data.data());\n",
    "thrust::tabulate(thrust::device, data.begin(), data.end(),\n",
    "                 [=] __host__ __device__ (size_t i0) {\n",
    "                     return data_ptr[i0] + 1;\n",
    "                 });\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae00fcd8-a494-4137-bf3e-57f74686628c",
   "metadata": {},
   "source": [
    "The full example code is available at [increase-thrust-expl.cu](../src/increase/increase-thrust-expl.cu) and [increase-thrust-mm.cu](../src/increase/increase-thrust-mm.cu).\n",
    "They can be build and executed using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d99cf5-f9a1-402d-a343-1254188f5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -std=c++17 --extended-lambda -arch=sm_86 -o ../build/increase/increase-thrust-expl ../src/increase/increase-thrust-expl.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a5203d-1456-494b-94f6-e691bce37d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-thrust-expl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e0cca-7136-4578-bd19-98f5eb5e0320",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -std=c++17 --extended-lambda -arch=sm_86 -o ../build/increase/increase-thrust-mm ../src/increase/increase-thrust-mm.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6d94f-6b8e-4ba8-be37-560efd51baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-thrust-mm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917661a6-d702-45ec-9ca1-f8f1e72e27d8",
   "metadata": {},
   "source": [
    "## Kokkos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7ebfd-74b4-4e2f-a6ca-bf71232e75ba",
   "metadata": {},
   "source": [
    "Kokkos provides its own abstraction for loops executed in parallel.\n",
    "Depending on how kokkos was compiled, this will map to either CPU or GPU execution spaces.\n",
    "\n",
    "```cpp\n",
    "Kokkos::parallel_for(\n",
    "    Kokkos::RangePolicy<>(0, nx),\n",
    "        KOKKOS_LAMBDA(const size_t i0) {\n",
    "            data(i0) += 1;\n",
    "        });\n",
    "```\n",
    "\n",
    "Tuning of the thread hierarchy composition can be done by specifying an additional *team policy*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7b22a2-4be2-4f7d-b045-18154e53a7ad",
   "metadata": {},
   "source": [
    "Synchronization for GPUs is not implicit but can be done by calling\n",
    "```cpp\n",
    "Kokkos::fence();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d586b971-0975-4aa1-ade8-f6069ee80df7",
   "metadata": {},
   "source": [
    "The full example code is available at [increase-kokkos.cpp](../src/increase/increase-kokkos.cpp).\n",
    "It can be build and executed using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2cd8d8-7f4b-4165-8fe3-7889fa22f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -O3 -march=native -std=c++17 -I/root/kokkos/install-serial/include -L/root/kokkos/install-serial/lib -o ../build/increase/increase-kokkos-serial ../src/increase/increase-kokkos.cpp -lkokkoscore -ldl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e51e0f-40da-4fb2-bc7a-24f514c6f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-kokkos-serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c712b811-0ef3-4191-afe3-61f0fc4fc696",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/root/kokkos/install-cuda/bin/nvcc_wrapper -O3 -march=native -std=c++17 -arch=sm_86 --expt-extended-lambda --expt-relaxed-constexpr -I/root/kokkos/install-cuda/include -L/root/kokkos/install-cuda/lib -o ../build/increase/increase-kokkos-cuda ../src/increase/increase-kokkos.cpp -lkokkoscore -ldl -lcuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af276d-9470-45eb-adf2-8772757ff5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-kokkos-cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90718e19-55c2-4b5c-9f54-1e168ef94dc0",
   "metadata": {},
   "source": [
    "## SYCL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bcbd81-aa22-459f-834f-5e1b6f76dc68",
   "metadata": {},
   "source": [
    "SYCL also provides its own abstraction for parallel loops, but additionally requires a *handler* and a *queue*.\n",
    "The following example assumes an already initialized `sycl::queue` named `q`.\n",
    "\n",
    "```cpp\n",
    "q.submit([&](sycl::handler &h) {\n",
    "    h.parallel_for(nx, [=](auto i0) {\n",
    "        data[i0] += 1;\n",
    "    });\n",
    "});\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5b95e-9a38-435b-9cda-a22ca0d68b77",
   "metadata": {},
   "source": [
    "Tuning the workgroup size is possible as well by providing global and local sizes, i.e. the total number of threads and the number of threads per workgroup.\n",
    "Note, that these two need to be evenly divisible and that any additionally spawned threads may need to be masked.\n",
    "\n",
    "```cpp\n",
    "auto local_size = 256;\n",
    "auto global_size = ceilingDivide(nx, local_size) * local_size;\n",
    "q.submit([&](sycl::handler &h) {\n",
    "    h.parallel_for( { global_size, local_size }\n",
    "        nx, [=](auto i0) {\n",
    "        if (i0 < nx) {\n",
    "            data[i0] += 1;\n",
    "        }\n",
    "    });\n",
    "});\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5695c290-f8e7-44e2-b999-d0524780faca",
   "metadata": {},
   "source": [
    "In any case, synchronization is done by calling\n",
    "```cpp\n",
    "q.wait();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cabc914-9445-4dbf-be4b-2f4df0674048",
   "metadata": {},
   "source": [
    "When using buffers, additional accessors must be created for accessing data\n",
    "\n",
    "```cpp\n",
    "q.submit([&](sycl::handler &h) {\n",
    "    auto data = b_data.get_access(h, sycl::read_write);\n",
    "\n",
    "    h.parallel_for(nx, [=](auto i0) {\n",
    "        dest[i0] = src[i0] + 1;\n",
    "    });\n",
    "});\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c16d8-82a3-4da8-ba90-c12ac5cec41d",
   "metadata": {},
   "source": [
    "The full example code is available at [increase-sycl-expl.cpp](../src/increase/increase-sycl-expl.cpp), [increase-sycl-mm.cpp](../src/increase/increase-sycl-mm.cpp) and [increase-sycl-buffer.cpp](../src/increase/increase-sycl-buffer.cpp).\n",
    "They can be build and executed using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8619ce12-b023-40d4-9db9-a7a0c23c864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!icpx -O3 -march=native -std=c++17 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_86 -o ../build/increase/increase-sycl-buffer ../src/increase/increase-sycl-buffer.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fc1c8-b18d-44e1-a853-e397e1f2fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-sycl-buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc0cb03-4af5-4161-87ea-7c6e4cf66007",
   "metadata": {},
   "outputs": [],
   "source": [
    "!icpx -O3 -march=native -std=c++17 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_86 -o ../build/increase/increase-sycl-expl ../src/increase/increase-sycl-expl.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c7b26-d518-4702-b15c-29d17a881d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-sycl-expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e590b96-5105-46a0-8f19-1a3d130d567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!icpx -O3 -march=native -std=c++17 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_86 -o ../build/increase/increase-sycl-mm ../src/increase/increase-sycl-mm.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675b009-cf18-47a3-9d64-01726e92be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-sycl-mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee23cf3-581b-4234-8e37-090cb15e08a8",
   "metadata": {},
   "source": [
    "## CUDA/ HIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2655e2-3628-4ea8-9e41-2369d36378ba",
   "metadata": {},
   "source": [
    "CUDA/HIP utilize separate *kernel* functions which are *launched* from the host code.\n",
    "By convention, they must return `void` and are marked with the `__global__` keyword.\n",
    "\n",
    "```cpp\n",
    "__global__ void increase(double* data, size_t nx) {\n",
    "    for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "        data[i0] += 1;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The kernel can be configured by providing an *execution configuration* in triple-chevron syntax.\n",
    "\n",
    "```cpp\n",
    "increase<<<1, 1>>>(d_data, nx);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae0f4d-26cb-4669-96bf-c74840c4a845",
   "metadata": {},
   "source": [
    "The above example runs on GPU, but all work is done in a single thread.\n",
    "Additional parallelization has to be done manually.\n",
    "One straight-forward way is assigning the work of a single loop iteration to a single thread, and to spawn as many threads as there are loop iterations.\n",
    "The mapping is done by having each thread evaluate the build-in thread variables to compute a unique global index or unique data index.\n",
    "\n",
    "```cpp\n",
    "__global__ void increase(double* data, size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    dest[i0] = src[i0] + 1;\n",
    "}\n",
    "```\n",
    "\n",
    "```cpp\n",
    "auto numThreadsPerBlock = 256;\n",
    "auto numBlocks = nx / numThreadsPerBlock;\n",
    "increase<<<numBlocks, numThreadsPerBlock>>>(d_data, nx);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2a5df-65f2-40b1-bf50-fd341e8a5182",
   "metadata": {},
   "source": [
    "While the above example works if nx is evenly divisible by the block size, it will not in all other cases.\n",
    "The most prominent way to fix this is to spawn an additional block and not have all threads of that block doing computations.\n",
    "\n",
    "```cpp\n",
    "__global__ void increase(double* data, size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (i0 < nx)\n",
    "        dest[i0] = src[i0] + 1;\n",
    "}\n",
    "```\n",
    "\n",
    "```cpp\n",
    "auto numThreadsPerBlock = 256;\n",
    "auto numBlocks = ceilingDivide(nx, numThreadsPerBlock);\n",
    "increase<<<numBlocks, numThreadsPerBlock>>>(d_data, nx);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457bbae-d6f6-4387-b7cd-5a7727badea4",
   "metadata": {},
   "source": [
    "The full example code is available at [increase-cuda-expl.cpp](../src/increase/increase-cuda-expl.cpp) and [increase-cuda-mm.cpp](../src/increase/increase-cuda-mm.cpp).\n",
    "They can be build and executed using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd095be7-a688-4bbb-8514-7f3b0577c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -std=c++17 -arch=sm_86 -o ../build/increase/increase-cuda-expl ../src/increase/increase-cuda-expl.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848961e-7902-4df3-a394-fed49763cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-cuda-expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a9ee2-1aca-4faa-a706-106b542e7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -std=c++17 -arch=sm_86 -o ../build/increase/increase-cuda-mm ../src/increase/increase-cuda-mm.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cd176-99f6-4400-a649-d838d0d17f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-cuda-mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd4ce7",
   "metadata": {},
   "source": [
    "## Next Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cfa87f",
   "metadata": {},
   "source": [
    "Head to the [next steps](./next-steps.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
