{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6715ec1e",
   "metadata": {},
   "source": [
    "# Profiling and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e8d000",
   "metadata": {},
   "source": [
    "Profiling is essential for understanding application performance and identifying optimization opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be518061",
   "metadata": {},
   "source": [
    "Nvidia provides robust profiling tools suitable for all GPU programming approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fec7df",
   "metadata": {},
   "source": [
    "A common and effective strategy is the *top-down approach* to performance analysis: start with a *whole application* overview, then focus on specific *hot spots*.\n",
    "\n",
    "Begin with Nsight Systems to get a broad performance overview, using either the command line or the GUI.\n",
    "Then, use Nsight Compute to analyze the performance of individual kernels in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2797dbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Nsight Systems Command Line Interface (CLI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af24f68d",
   "metadata": {},
   "source": [
    "First, compile and run the benchmark application to ensure it produces correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b77e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -std=c++17 -arch=sm_86 -o ../build/increase/increase-cuda-expl ../src/increase/increase-cuda-expl.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3497725a",
   "metadata": {},
   "source": [
    "Next, profile the binary using `nsys profile`.\n",
    "\n",
    "Key command line arguments:\n",
    "* `--stats=true`: Prints a summary of performance statistics to the command line\n",
    "* `-o ...`: Specifies the output profile file\n",
    "* `--force-overwrite=true`: Overwrites the profile file if it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab15e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --stats=true -o ../profiles/increase-cuda-expl --force-overwrite=true ../build/increase/increase-cuda-expl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7e250d",
   "metadata": {},
   "source": [
    "The most relevant sections in the profiling output are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe60ea",
   "metadata": {},
   "source": [
    "**CUDA API Statistics** (`cuda_api_sum`)\n",
    "\n",
    "This section provides timing details for CUDA API calls such as `cudaMalloc`, `cudaMemcpy`, and `cudaDeviceSynchronize`.\n",
    "\n",
    "For each API function, Nsight Systems reports:\n",
    "* The relative and absolute total time spent\n",
    "* The number of calls\n",
    "* Statistics (min, max, average, median) for call durations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fda128",
   "metadata": {},
   "source": [
    "**CUDA Kernel Statistics** (`cuda_gpu_kern_sum`)\n",
    "\n",
    "This section reports, for each kernel:\n",
    "* The relative and absolute total execution time\n",
    "* The number of launches\n",
    "* Statistics (min, max, average, median) for kernel execution times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fac5e32",
   "metadata": {},
   "source": [
    "**Memory Transfers** (`cuda_gpu_mem_time_sum` and `cuda_gpu_mem_size_sum`)\n",
    "\n",
    "These two sections focus on data transfers between host and device, reporting:\n",
    "* The total time spent on transfers per direction, with statistics\n",
    "* The total amount of data transferred, with statistics for individual transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da673d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Nsight Systems Graphical User Interface (GUI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144cd3bc",
   "metadata": {},
   "source": [
    "To further investigate application behavior, open the generated `increase-cuda-expl.nsys-rep` report file in the `../profiles` folder **on your local PC**.\n",
    "\n",
    "Download the file using the file browser on the left, or **shift** + **right-click** on this [link](../profiles/increase-cuda-expl.nsys-rep) and choose *Save Link As*.\n",
    "Then, open the file with your local installation of Nsight Systems (version 2025.1.1 or newer required)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f08c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Nsight Compute Command Line Interface (CLI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f8cd8",
   "metadata": {},
   "source": [
    "After addressing system-level performance issues and identifying hot-spot kernels, the next step is to profile the application in detail using the Nsight Compute CLI (`ncu`).\n",
    "\n",
    "Key command line arguments:\n",
    "* `-o ...`: Specifies the output profile file (similar to `nsys`)\n",
    "* `--force-overwrite`: Overwrites the profile file if it already exists (without `=true`, unlike `nsys`)\n",
    "\n",
    "You can further limit which kernels are profiled with:\n",
    "* `--launch-skip n` or `-s n`: Skips the first `n` kernels\n",
    "* `--launch-count n` or `-c n`: Profiles only the first `n` applicable kernels\n",
    "* `--kernel name` or `-k name`: Profiles only kernels with the specified name (supports regex and kernel renaming)\n",
    "\n",
    "See the [documentation](https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html#profile) for a full list of arguments.\n",
    "\n",
    "If no output file is specified, results are printed to the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb507496",
   "metadata": {},
   "source": [
    "Note: If you do not restrict which kernels are profiled (e.g., by setting the launch count), *every kernel* will be profiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f812797",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu -s 2 -c 1  ../build/increase/increase-cuda-expl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5477e7",
   "metadata": {},
   "source": [
    "When profiling on a remote machine and analyzing results locally, it is often beneficial to collect more data than strictly necessary.\n",
    "You can do this by adding the `--set=full` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa26fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu -s 2 -c 1 --set=full -o ../profiles/increase-cuda-expl --force-overwrite ../build/increase/increase-cuda-expl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0054474e",
   "metadata": {},
   "source": [
    "As before, download the resulting file (`increase-cuda-expl.ncu-rep`) using the file browser on the left or **shift** + **right-click** on this [link](../profiles/increase-cuda-expl.ncu-rep) and choose *Save Link As* to open it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f553db19",
   "metadata": {},
   "source": [
    "A comprehensive discussion of this topic is beyond this tutorial's scope.\n",
    "For more details, see for example the NHR@FAU course *GPU Performance Engineering*.\n",
    "The next workshop data will be announced at [https://hpc.fau.de/teaching/tutorials-and-courses](https://hpc.fau.de/teaching/tutorials-and-courses)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6232a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Optimization Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da56fd1f",
   "metadata": {},
   "source": [
    "Using the performance evaluation techniques introduced above, you can identify different performance patterns.\n",
    "These patterns not only guide optimization strategies, but also inform the choice of GPU programming approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e84ebe",
   "metadata": {},
   "source": [
    "**Majority of Time Spent in Device Synchronization**\n",
    "\n",
    "While this often means the GPU is doing useful work, it may also indicate that additional, independent CPU work could be overlapped.\n",
    "Achieving this requires fully asynchronous kernel execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8cd80",
   "metadata": {},
   "source": [
    "**Low Degree of Parallelism**\n",
    "\n",
    "A single kernel may not fully utilize the GPU's capabilities.\n",
    "This is often reflected in low *occupancy*, which can sometimes be estimated from hardware characteristics.\n",
    "For example, if the number of CUDA blocks is less than the number of available SMs, or if block sizes are very small, parallelism is likely insufficient.\n",
    "\n",
    "To address this, consider restructuring the computation to expose more parallelism, or run multiple kernels in parallel.\n",
    "This may require modeling dependencies between kernels and using finer-grained synchronization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc95e5",
   "metadata": {},
   "source": [
    "**Data Transfers between Host and Device Dominate**\n",
    "\n",
    "If your application uses managed memory, first consider applying prefetching.\n",
    "If prefetching is not supported by your programming approach, switching to explicit memory management may help.\n",
    "\n",
    "Another optimization is to overlap data transfers and kernel execution.\n",
    "This requires fully asynchronous kernel execution, asynchronous memory transfers, and appropriate synchronization or dependency primitives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c6d44",
   "metadata": {},
   "source": [
    "**Data Sharing and Aggregation across Groups of Threads**\n",
    "\n",
    "In many applications, threads within a group (block, team, gang, work-group, etc.) repeatedly access the same data elements.\n",
    "While hardware caching often helps, manual buffering can sometimes be more efficient.\n",
    "\n",
    "Using SM-local *shared memory* enables both efficient data sharing and synchronization among threads in a group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
