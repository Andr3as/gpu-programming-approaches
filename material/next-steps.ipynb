{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a45930eb-71cc-48b6-a996-4e38943b5b49",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab9859f-6405-4084-aef8-432a856adcd0",
   "metadata": {},
   "source": [
    "## Profiling and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b82076b-d1b9-4f9f-ba31-481fe10f120c",
   "metadata": {},
   "source": [
    "Nvidia provides mature profiling tools that can be used for all approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72137997-60b5-402e-9f8e-270a91e40eb8",
   "metadata": {},
   "source": [
    "A popular pattern is following a *top-down approach* to performance analysis, starting with a *whole application* performance overview and then narrowing down to specific *hot spots*.\n",
    "The initial overview can be obtained using Nsight Systems, using either solely the command line interface, or by complementing the analysis with the provided GUI.\n",
    "Nsight Compute can then be used to evaluate the performance of single kernels of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4117ef8-05f9-40be-89d8-48797ff03f25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nsight Systems CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d8f1b-97d0-4b97-864b-acc4fc9db2c9",
   "metadata": {},
   "source": [
    "First, we compile and execute out benchmark application to make sure that results are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95096440-6ee7-449b-a0bc-a3963ef818d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -std=c++17 -arch=sm_86 -o ../build/increase/increase-cuda-expl ../src/increase/increase-cuda-expl.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0bcdca-519d-47a8-a334-0c0cd08e14f0",
   "metadata": {},
   "source": [
    "Next, we profile our binary with `nsys profile`.\n",
    "Further command line arguments are:\n",
    "* `--stats=true`: prints a summary of performance statistics on the command line\n",
    "* `-o ...`: sets the target output profile file\n",
    "* `--force-overwrite=true`: replaces the profile file if it already exists (instead of aborting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b8e14-bad9-4f3f-9e13-c67ef7bb304b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nsys profile --stats=true -o ../profiles/increase-cuda-expl --force-overwrite=true ../build/increase/increase-cuda-expl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c2d1b-3944-4bb0-b323-40838a543388",
   "metadata": {},
   "source": [
    "The most relevant sections are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fc6296-d764-4386-b80b-92e311989b4e",
   "metadata": {},
   "source": [
    "**CUDA API Statistics** (`cuda_api_sum`)\n",
    "\n",
    "This section provides timing details for calls to the CUDA API such as `cudaMalloc`, `cudaMemcpy`, `cudaDeviceSynchronize`, etc.\n",
    "\n",
    "For each API function called, nsight system reports\n",
    "* the relative and absolute total (aggregated) time spent in its execution\n",
    "* the number of calls, and\n",
    "* statistics (min, max, average and median) about each calls duration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d1808-9907-4073-aa5e-c27b7df2cc59",
   "metadata": {},
   "source": [
    "**CUDA Kernel Statistics** (`cuda_gpu_kern_sum`)\n",
    "\n",
    "This section reports for each kernel\n",
    "* the relative and absolute total (aggregated) time spent in its execution,\n",
    "* the number of launches, and\n",
    "* statistics (min, max, average and median) of the kernel execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d6b2c1-ecdf-4f24-b8b3-7475000d76c8",
   "metadata": {},
   "source": [
    "**Memory Transfers** (`cuda_gpu_mem_time_sum` and `cuda_gpu_mem_size_sum`)\n",
    "\n",
    "These two sections focus on data transfers between host and device and report\n",
    "* the total time spent in transfers per direction, and their statistics, and\n",
    "* the total amount of data transferred, and statistics about the individual transfers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d959c0de-13e0-4b02-a22b-9711c2d82d2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nsight Systems GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689cb4a5-7bb5-4fce-a803-bbaf6db0d3d4",
   "metadata": {},
   "source": [
    "We can further investigate the application behavior by opening up the generated `increase-cuda-expl.nsys-rep` report file which has been saved to the `../profiles` folder.\n",
    "\n",
    "To do so, download the file using the file browser on the left or **shift** + **right-click** on this [link](../profiles/increase-cuda-expl.nsys-rep) and choose *save link as*.\n",
    "Next, open the downloaded file with your local installation of Nsight Systems (requires version 2025.1.1 or newer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481e7c1-7deb-4e2f-ac1d-3740679b4fb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nsight Compute CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bc2bd2-6364-4c53-9e6b-efd20abccaaf",
   "metadata": {},
   "source": [
    "After fixing system level performance issues, and isolating hot-spots kernels, next is profiling the application using the CLI of Nsight Compute: `ncu`.\n",
    "\n",
    "Its command line arguments are:\n",
    "* `-o ...`: sets the target output profile file (equivalent to `nsys`)\n",
    "* `--force-overwrite`: replaces the profile file if it already exists (in contrast to `nsys` no `=true`)\n",
    "\n",
    "We can further limit the scope of profiled kernels with\n",
    "* `--launch-skip n` or `-s n`: skips the first `n` kernels encountered\n",
    "* `--launch-count n` or `-c n`: limits profiling to the first `n` applicable kernels\n",
    "* `--kernel name` or `-k name`: limits profiling to kernels with the name `name`\n",
    "  * can also be used with regex\n",
    "  * Nsight Compute also supports kernel renaming\n",
    "\n",
    "All command line arguments are listed in the [documentation](https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html#profile).\n",
    "\n",
    "Without an output file, results are printed to the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca3c60-2e1f-4fa9-961a-41d9e749da0a",
   "metadata": {},
   "source": [
    "Note that we also decrease the number of iterations since by default *every kernel* is profiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8421f0-c0d7-4841-889a-651e7880659b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ncu -s 2 -c 1  ../build/increase/increase-cuda-expl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e6292-8f91-4ec7-9a59-33024cb1e4c7",
   "metadata": {},
   "source": [
    "When profiling on a remote machine while the GUI to evaluate the results is run locally, it often is advantageous to obtain more data than might be necessary.\n",
    "One easy way to realize this behavior is adding the `--set=full` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67c3de-3d7e-47b8-82c1-b8c3b308e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu -s 2 -c 1 --set=full -o ../profiles/increase-cuda-expl --force-overwrite ../build/increase/increase-cuda-expl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc6476-2a48-4525-96ec-1384554c640c",
   "metadata": {},
   "source": [
    "As before, the resulting file (`increase-cuda-expl.ncu-rep`) can opened locally after being downloaded using the file browser on the left or with **shift** + **right-click** on this [link](../profiles/increase-cuda-expl.ncu-rep) and *save link as*.\n",
    "\n",
    "A more in-depth look into GPU performance engineering is beyond the scope of this tutorial, but NHR@FAU offers a comprehensive course on this topic.\n",
    "Details can be found at [https://hpc.fau.de/teaching/tutorials-and-courses](https://hpc.fau.de/teaching/tutorials-and-courses)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7193e-f194-4679-827d-8c6af74765d1",
   "metadata": {},
   "source": [
    "### Optimization Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00d558-c73a-4e75-a0f9-d8a1d933b3d2",
   "metadata": {},
   "source": [
    "Using the previously (shallowly) introduced performance evaluation techniques different patterns can be identified.\n",
    "Generally, these patterns guide optimization strategies, but, more relevant to the scope of this workshop, also the choice of GPU programming approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c24b5-ad3b-4bbd-97d7-76b0a44952e4",
   "metadata": {},
   "source": [
    "**Majority of Time Spent in Device Synchronization**\n",
    "\n",
    "While this is generally a good thing (the GPU is doing meaningful work during this time), it can also be a hint that overlapping additional, independent work on the CPU is possible.\n",
    "In order to achieve this, fully asynchronous kernel execution must be supported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18862cec-c92e-44aa-8967-6e8c8a4a2559",
   "metadata": {},
   "source": [
    "**Low Degree of Parallelism**\n",
    "\n",
    "A single kernel is not able to fully saturate the capabilities of the GPU it is executed on.\n",
    "This frequently shows in a low *occupancy*, and can in many cases also be approximated from the hardware characteristics.\n",
    "In particular, a number of CUDA blocks lower than the number of SMs available, and/ or a very low block size are good indicators.\n",
    "\n",
    "Apart from attempting to restructure the computation to expose a higher degree of parallelism, one alternative remedy is running multiple kernels in parallel.\n",
    "This requires the ability to model dependencies between kernels and thereby the the expression of potential overlap.\n",
    "More fine-grained synchronization mechanics can also be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce51e95-69df-493f-913c-8d9186582903",
   "metadata": {},
   "source": [
    "**Data Transfers between Host and Device Dominate**\n",
    "\n",
    "If the application relies on managed memory, the first step is looking into the potential of applying prefetching.\n",
    "For cases where this concept is not supported by the chosen GPU programming approach, switching to explicitly handled memory can be worthwhile.\n",
    "\n",
    "Another potential optimization lies in overlapping data transfers and kernel execution.\n",
    "As before, this requires fully asynchronous kernel execution, but in addition to that also fully asynchronous memory transfers as well as synchronization or dependency primitives between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768dc9c3-6749-466f-a13d-7488ad39d0c6",
   "metadata": {},
   "source": [
    "**Data Sharing and Aggregation across Groups of Threads**\n",
    "\n",
    "In many applications, the threads of a given group (block, team, gang, work-group, ...) access the same data elements multiple times.\n",
    "While caching is usually able to handle this quite well, in some cases manually buffering data is the better choice.\n",
    "\n",
    "Using the SM-local *shared memory* allows exactly that and, additionally, also synchronization between all threads of a group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0666123-eee7-48d4-a33a-ab1cc4393567",
   "metadata": {},
   "source": [
    "## Computational Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc81e22",
   "metadata": {},
   "source": [
    "When choosing a GPU programming approach for an application at hand it can be vital to first identify *computational patterns*, and check for their support.\n",
    "* Can they be implemented at all?\n",
    "* Can they be implemented concisely?\n",
    "* Can they be implemented efficiently?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ea919",
   "metadata": {},
   "source": [
    "One frequently occurring pattern are *reductions*, e.g. summing all elements of a vector, computing a dot product, or finding the minimum value in an array.\n",
    "The key issue when parallelizing reductions lies in the naturally occurring race condition.\n",
    "Consider the following CPU function and corresponding kernel:\n",
    "\n",
    "```cpp\n",
    "void reduce(double *data, size_t nx) {\n",
    "    double sum = 0;\n",
    "\n",
    "    for (size_t i0 = 0; i0 < nx; ++i0)\n",
    "        sum += data[i0];\n",
    "\n",
    "    return sum;\n",
    "}\n",
    "```\n",
    "\n",
    "```cpp\n",
    "__global__ void reduce(double *data, double* sum, size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (i0 < nx)\n",
    "        *sum += data[i0];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc73e2",
   "metadata": {},
   "source": [
    "The compound assignment looks like a single operation, but in practice it requires multiple steps:\n",
    "* loading the old value of `sum` from memory and storing it in an intermediate variable (e.g. `tmp`),\n",
    "* adding `data[i0]` to `tmp`, and\n",
    "* writing back the value of `tmp` to `sum`.\n",
    "\n",
    "When these operations are executed in parallel, some updates may be lost:\n",
    "* Multiple threads read `sum` concurrently, then\n",
    "* they modify their local versions concurrently, and\n",
    "* lastly, they write back their computational result, thereby overwriting the contributions of other concurrent threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35badf9c",
   "metadata": {},
   "source": [
    "One option to fix this is making sure that the compound assignment is realized as a single *atomic* operation.\n",
    "\n",
    "```cpp\n",
    "__global__ void reduce(double *data, double* sum, size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (i0 < nx)\n",
    "        atomicAdd(sum, data[i0]);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb1ba3",
   "metadata": {},
   "source": [
    "While this version works correctly, performance can be sub-optimal due to *atomic congestion*.\n",
    "Further optimization is possible by performing a *hierarchical reduction* via one or more of the following adaptations:\n",
    "* Each thread computes the sum across multiple input values.\n",
    "* Each warp performs a reduction across its threads.\n",
    "* Each block performs a reduction across its threads/ warps.\n",
    "\n",
    "Discussing implementation details for all variants is beyond the scope of this tutorial.\n",
    "One accessible option, however, is implementing a block reduction using `cub`, a header-only library shipped as part of the Nvidia HPC Toolkit (or using `hipCUB` on AMD).\n",
    "\n",
    "```cpp\n",
    "#include <cub/cub.cuh>\n",
    "\n",
    "template <unsigned int blockSize>\n",
    "__global__ void reduce(double *data, double* sum, size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    // define BlockReduce type with as many threads per block as we use\n",
    "    typedef cub::BlockReduce <DATA_TYPE, blockSize, cub::BLOCK_REDUCE_RAKING_COMMUTATIVE_ONLY> BlockReduce;\n",
    "\n",
    "    // allocate shared memory for block reduction\n",
    "    __shared__ typename BlockReduce::TempStorage tempStorage;\n",
    "\n",
    "    double elem = 0;\n",
    "\n",
    "    if (i0 < nx)\n",
    "        elem = data[i0];\n",
    "\n",
    "    // reduce over block (all threads must participate)\n",
    "    double blockSum = BlockReduce(tempStorage).Sum(elem);\n",
    "\n",
    "    // atomically add the result to the global sum\n",
    "    if (0 == threadIdx.x && i0 < numElements)\n",
    "        atomicAdd(sum, blockSum);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548414f",
   "metadata": {},
   "source": [
    "Other approaches offer reductions as well - with varying programming effort, flexibility, and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc12ed0",
   "metadata": {},
   "source": [
    "### OpenMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b379d980",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double sum = 0;\n",
    "\n",
    "#pragma omp target teams distribute parallel for \\\n",
    "            reduction(+ : sum)\n",
    "for (size_t i0 = 0; i0 < nx; ++i0)\n",
    "    sum += data[i0];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb355b2",
   "metadata": {},
   "source": [
    "### OpenACC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f364d552",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double sum = 0;\n",
    "\n",
    "#pragma acc parallel loop present(data[:nx]) \\\n",
    "            reduction(+ : sum)\n",
    "for (size_t i0 = 0; i0 < nx; ++i0)\n",
    "    sum += data[i0];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2284f",
   "metadata": {},
   "source": [
    "### Modern C++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dcd64f",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double sum = std::reduce(std::execution::par_unseq, data, data + nx, 0., std::plus{});\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c073ff",
   "metadata": {},
   "source": [
    "### Thrust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce68cad8",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double sum = thrust::reduce(data, data + nx, 0.);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb5169",
   "metadata": {},
   "source": [
    "### Kokkos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739f74b",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double sum = 0;\n",
    "\n",
    "Kokkos::parallel_reduce(\n",
    "    Kokkos::RangePolicy<>(0, nx),\n",
    "    KOKKOS_LAMBDA(const size_t i0, double &acc) {\n",
    "        acc += data(i0);\n",
    "    }, sum);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d62ad87",
   "metadata": {},
   "source": [
    "### SYCL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724469a5",
   "metadata": {},
   "source": [
    "```cpp\n",
    "q.submit([&](sycl::handler &h) {\n",
    "    h.parallel_for(nx, [=](auto i0) {\n",
    "        auto v = sycl::atomic_ref<double, sycl::memory_order::relaxed,\n",
    "                                  sycl::memory_scope::device,\n",
    "                                  sycl::access::address_space::global_space>(\n",
    "            sum[0]);\n",
    "        v.fetch_add(data[i0]);\n",
    "    });\n",
    "});\n",
    "```\n",
    "\n",
    "Additional optimization, similar to CUDA, can be implemented as well.\n",
    "For this, [Intel's OneAPI optimization guide](https://www.intel.com/content/www/us/en/docs/oneapi/optimization-guide-gpu/2025-0/reduction.html) is a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e40977",
   "metadata": {},
   "source": [
    "### Additional Consideration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52f92de",
   "metadata": {},
   "source": [
    "In many cases, reducing over a set of values can be fused with their production.\n",
    "Consider, e.g., computing a dot product.\n",
    "A naive implementation is done in two steps:\n",
    "* The result of a point-wise multiplication is stored in a temporary vector\n",
    "* A sum reduction is applied to the temporary vector\n",
    "\n",
    "Fusing both steps not only reduces the memory footprint, but is also expected to improve performance since less data needs to be read/ written."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b4600",
   "metadata": {},
   "source": [
    "For most approaches discussed previously, this fusion is not difficult to implement.\n",
    "Two slight exceptions are modern C++ and Thrust - here different algorithms need to be used:\n",
    "* `std::transform_reduce`\n",
    "* `thrust::transform_reduce` or `thrust::transform_iterator`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f98484f",
   "metadata": {},
   "source": [
    "## Beyond 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59ee5ba-fd5b-4b20-8d54-e79b884ae05a",
   "metadata": {},
   "source": [
    "Many algorithms don't build on 1D iteration spaces and data structures, but instead on multidimensional ones.\n",
    "Key considerations when choosing a GPU programming approach are\n",
    "* Can nD iteration spaces be parallelized intuitively?\n",
    "* Can the different parts of the thread hierarchy be multidimensional? This can boost data reuse for data access patterns following a concept of neighborhood.\n",
    "* Is there support for multidimensional data structures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b9a90-cd5a-4f49-ab35-c3dae9703229",
   "metadata": {},
   "source": [
    "## Interoperability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d885722-2c66-484d-a301-ec90d2ba26b2",
   "metadata": {},
   "source": [
    "Another topic to consider when comparing different GPU programming approaches is their interoperability.\n",
    "Many of them expose an interface from and to CUDA/HIP on the corresponding platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1da9f2-96d4-4cd7-b6b9-eb1caad6f03c",
   "metadata": {},
   "source": [
    "## Multi-GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05208088-d364-4f11-ae62-7d908ff38fc2",
   "metadata": {},
   "source": [
    "Last but not least, scaling workloads to multiple GPUs and even multiple GPU-equipped compute nodes is a relevant topic in many HPC application areas.\n",
    "\n",
    "This requires concepts for targeting different GPUs available on the current node, as well as interoperability with distributed memory parallelization solutions such as MPI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff3f5e8",
   "metadata": {},
   "source": [
    "## Next Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729262de",
   "metadata": {},
   "source": [
    "Head to the [programming challenge](./programming-challenge.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
