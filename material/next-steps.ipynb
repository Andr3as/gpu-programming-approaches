{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a45930eb-71cc-48b6-a996-4e38943b5b49",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea139f1",
   "metadata": {},
   "source": [
    "\n",
    "In this section, we outline how to analyze and optimize GPU applications using Nvidia's profiling tools, and discuss patterns and considerations for further performance improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab9859f-6405-4084-aef8-432a856adcd0",
   "metadata": {},
   "source": [
    "## Profiling and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c4ce7c",
   "metadata": {},
   "source": [
    "Profiling is essential for understanding application performance and identifying optimization opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b82076b-d1b9-4f9f-ba31-481fe10f120c",
   "metadata": {},
   "source": [
    "Nvidia provides robust profiling tools suitable for all GPU programming approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72137997-60b5-402e-9f8e-270a91e40eb8",
   "metadata": {},
   "source": [
    "A common and effective strategy is the *top-down approach* to performance analysis: start with a *whole application* overview, then focus on specific *hot spots*.\n",
    "\n",
    "Begin with Nsight Systems to get a broad performance overview, using either the command line or the GUI.\n",
    "Then, use Nsight Compute to analyze the performance of individual kernels in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4117ef8-05f9-40be-89d8-48797ff03f25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nsight Systems Command Line Interface (CLI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d8f1b-97d0-4b97-864b-acc4fc9db2c9",
   "metadata": {},
   "source": [
    "First, compile and run the benchmark application to ensure it produces correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95096440-6ee7-449b-a0bc-a3963ef818d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -std=c++17 -arch=sm_86 -o ../build/increase/increase-cuda-expl ../src/increase/increase-cuda-expl.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0bcdca-519d-47a8-a334-0c0cd08e14f0",
   "metadata": {},
   "source": [
    "Next, profile the binary using `nsys profile`.\n",
    "\n",
    "Key command line arguments:\n",
    "* `--stats=true`: Prints a summary of performance statistics to the command line\n",
    "* `-o ...`: Specifies the output profile file\n",
    "* `--force-overwrite=true`: Overwrites the profile file if it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b8e14-bad9-4f3f-9e13-c67ef7bb304b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nsys profile --stats=true -o ../profiles/increase-cuda-expl --force-overwrite=true ../build/increase/increase-cuda-expl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c2d1b-3944-4bb0-b323-40838a543388",
   "metadata": {},
   "source": [
    "The most relevant sections in the profiling output are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fc6296-d764-4386-b80b-92e311989b4e",
   "metadata": {},
   "source": [
    "**CUDA API Statistics** (`cuda_api_sum`)\n",
    "\n",
    "This section provides timing details for CUDA API calls such as `cudaMalloc`, `cudaMemcpy`, and `cudaDeviceSynchronize`.\n",
    "\n",
    "For each API function, Nsight Systems reports:\n",
    "* The relative and absolute total time spent\n",
    "* The number of calls\n",
    "* Statistics (min, max, average, median) for call durations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d1808-9907-4073-aa5e-c27b7df2cc59",
   "metadata": {},
   "source": [
    "**CUDA Kernel Statistics** (`cuda_gpu_kern_sum`)\n",
    "\n",
    "This section reports, for each kernel:\n",
    "* The relative and absolute total execution time\n",
    "* The number of launches\n",
    "* Statistics (min, max, average, median) for kernel execution times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d6b2c1-ecdf-4f24-b8b3-7475000d76c8",
   "metadata": {},
   "source": [
    "**Memory Transfers** (`cuda_gpu_mem_time_sum` and `cuda_gpu_mem_size_sum`)\n",
    "\n",
    "These two sections focus on data transfers between host and device, reporting:\n",
    "* The total time spent on transfers per direction, with statistics\n",
    "* The total amount of data transferred, with statistics for individual transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d959c0de-13e0-4b02-a22b-9711c2d82d2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nsight Systems Graphical User Interface (GUI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689cb4a5-7bb5-4fce-a803-bbaf6db0d3d4",
   "metadata": {},
   "source": [
    "To further investigate application behavior, open the generated `increase-cuda-expl.nsys-rep` report file in the `../profiles` folder *on you notebook*.\n",
    "\n",
    "Download the file using the file browser on the left, or **shift** + **right-click** on this [link](../profiles/increase-cuda-expl.nsys-rep) and choose *Save Link As*.\n",
    "Then, open the file with your local installation of Nsight Systems (version 2025.1.1 or newer required)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481e7c1-7deb-4e2f-ac1d-3740679b4fb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nsight Compute Command Line Interface (CLI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bc2bd2-6364-4c53-9e6b-efd20abccaaf",
   "metadata": {},
   "source": [
    "After addressing system-level performance issues and identifying hot-spot kernels, the next step is to profile the application in detail using the Nsight Compute CLI (`ncu`).\n",
    "\n",
    "Key command line arguments:\n",
    "* `-o ...`: Specifies the output profile file (similar to `nsys`)\n",
    "* `--force-overwrite`: Overwrites the profile file if it already exists (without `=true`, unlike `nsys`)\n",
    "\n",
    "You can further limit which kernels are profiled with:\n",
    "* `--launch-skip n` or `-s n`: Skips the first `n` kernels\n",
    "* `--launch-count n` or `-c n`: Profiles only the first `n` applicable kernels\n",
    "* `--kernel name` or `-k name`: Profiles only kernels with the specified name (supports regex and kernel renaming)\n",
    "\n",
    "See the [documentation](https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html#profile) for a full list of arguments.\n",
    "\n",
    "If no output file is specified, results are printed to the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca3c60-2e1f-4fa9-961a-41d9e749da0a",
   "metadata": {},
   "source": [
    "Note: If you do not restrict which kernels are profiled (e.g., by setting the launch count), *every kernel* will be profiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8421f0-c0d7-4841-889a-651e7880659b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ncu -s 2 -c 1  ../build/increase/increase-cuda-expl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e6292-8f91-4ec7-9a59-33024cb1e4c7",
   "metadata": {},
   "source": [
    "When profiling on a remote machine and analyzing results locally, it is often beneficial to collect more data than strictly necessary.\n",
    "You can do this by adding the `--set=full` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67c3de-3d7e-47b8-82c1-b8c3b308e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu -s 2 -c 1 --set=full -o ../profiles/increase-cuda-expl --force-overwrite ../build/increase/increase-cuda-expl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc6476-2a48-4525-96ec-1384554c640c",
   "metadata": {},
   "source": [
    "As before, download the resulting file (`increase-cuda-expl.ncu-rep`) using the file browser on the left or **shift** + **right-click** on this [link](../profiles/increase-cuda-expl.ncu-rep) and choose *Save Link As* to open it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9555c0b8",
   "metadata": {},
   "source": [
    "A comprehensive discussion of this topic is beyond this tutorial's scope.\n",
    "For more details, see for example the NHR@FAU course *GPU Performance Engineering*.\n",
    "The next workshop data will be announced at [https://hpc.fau.de/teaching/tutorials-and-courses](https://hpc.fau.de/teaching/tutorials-and-courses)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7193e-f194-4679-827d-8c6af74765d1",
   "metadata": {},
   "source": [
    "### Optimization Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00d558-c73a-4e75-a0f9-d8a1d933b3d2",
   "metadata": {},
   "source": [
    "Using the performance evaluation techniques introduced above, you can identify different performance patterns.\n",
    "These patterns not only guide optimization strategies, but also inform the choice of GPU programming approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c24b5-ad3b-4bbd-97d7-76b0a44952e4",
   "metadata": {},
   "source": [
    "**Majority of Time Spent in Device Synchronization**\n",
    "\n",
    "While this often means the GPU is doing useful work, it may also indicate that additional, independent CPU work could be overlapped.\n",
    "Achieving this requires fully asynchronous kernel execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18862cec-c92e-44aa-8967-6e8c8a4a2559",
   "metadata": {},
   "source": [
    "**Low Degree of Parallelism**\n",
    "\n",
    "A single kernel may not fully utilize the GPU's capabilities.\n",
    "This is often reflected in low *occupancy*, which can sometimes be estimated from hardware characteristics.\n",
    "For example, if the number of CUDA blocks is less than the number of available SMs, or if block sizes are very small, parallelism is likely insufficient.\n",
    "\n",
    "To address this, consider restructuring the computation to expose more parallelism, or run multiple kernels in parallel.\n",
    "This may require modeling dependencies between kernels and using finer-grained synchronization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce51e95-69df-493f-913c-8d9186582903",
   "metadata": {},
   "source": [
    "**Data Transfers between Host and Device Dominate**\n",
    "\n",
    "If your application uses managed memory, first consider applying prefetching.\n",
    "If prefetching is not supported by your programming approach, switching to explicit memory management may help.\n",
    "\n",
    "Another optimization is to overlap data transfers and kernel execution.\n",
    "This requires fully asynchronous kernel execution, asynchronous memory transfers, and appropriate synchronization or dependency primitives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768dc9c3-6749-466f-a13d-7488ad39d0c6",
   "metadata": {},
   "source": [
    "**Data Sharing and Aggregation across Groups of Threads**\n",
    "\n",
    "In many applications, threads within a group (block, team, gang, work-group, etc.) repeatedly access the same data elements.\n",
    "While hardware caching often helps, manual buffering can sometimes be more efficient.\n",
    "\n",
    "Using SM-local *shared memory* enables both efficient data sharing and synchronization among threads in a group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0666123-eee7-48d4-a33a-ab1cc4393567",
   "metadata": {},
   "source": [
    "## Computational Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc81e22",
   "metadata": {},
   "source": [
    "When selecting a GPU programming approach, it is crucial to identify the application's *computational patterns* and check for their support:\n",
    "* Can the pattern be implemented at all?\n",
    "* Can it be implemented concisely?\n",
    "* Can it be implemented efficiently?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ea919",
   "metadata": {},
   "source": [
    "A common pattern is the *reduction*, such as summing all elements of a vector, computing a dot product, or finding a minimum value in an array.\n",
    "\n",
    "The main challenge in parallelizing reductions is the inherent race condition.\n",
    "Consider the following CPU function and its corresponding kernel:\n",
    "\n",
    "```cpp\n",
    "void reduce(double *data, size_t nx) {\n",
    "    double sum = 0;\n",
    "\n",
    "    for (size_t i0 = 0; i0 < nx; ++i0)\n",
    "        sum += data[i0];\n",
    "\n",
    "    return sum;\n",
    "}\n",
    "```\n",
    "\n",
    "```cpp\n",
    "__global__ void reduce(double *data, double* sum, size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (i0 < nx)\n",
    "        *sum += data[i0];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc73e2",
   "metadata": {},
   "source": [
    "The compound assignment appears like a single operation, but actually involves several steps:\n",
    "* Loading the old value of `sum` from memory into a temporary variable (e.g., `tmp`)\n",
    "* Adding `data[i0]` to `tmp`\n",
    "* Writing the value of `tmp` back to `sum`\n",
    "\n",
    "When these steps are performed in parallel, some updates may be lost:\n",
    "* Multiple threads read `sum` concurrently\n",
    "* Each modifies its local copy\n",
    "* They write back their results, potentially overwriting each other's contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35badf9c",
   "metadata": {},
   "source": [
    "One solution is to ensure the compound assignment is performed as a single *atomic* operation:\n",
    "\n",
    "```cpp\n",
    "__global__ void reduce(double *data, double* sum, size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (i0 < nx)\n",
    "        atomicAdd(sum, data[i0]);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb1ba3",
   "metadata": {},
   "source": [
    "While this version is correct, performance may suffer due to *atomic congestion*.\n",
    "Further optimization is possible through *hierarchical reduction*, which can include:\n",
    "* Each thread summing multiple input values\n",
    "* Each warp performing a reduction across its threads\n",
    "* Each block performing a reduction across its threads or warps\n",
    "\n",
    "A full discussion of all variants is beyond this tutorial, but a practical option is to use a block reduction with `cub`, a header-only library included in the Nvidia HPC Toolkit (or `hipCUB` on AMD):\n",
    "\n",
    "```cpp\n",
    "#include <cub/cub.cuh>\n",
    "\n",
    "template <unsigned int blockSize>\n",
    "__global__ void reduce(double *data, double* sum, size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    // Define BlockReduce type for the block size\n",
    "    typedef cub::BlockReduce <DATA_TYPE, blockSize, cub::BLOCK_REDUCE_RAKING_COMMUTATIVE_ONLY> BlockReduce;\n",
    "\n",
    "    // Allocate shared memory for block reduction\n",
    "    __shared__ typename BlockReduce::TempStorage tempStorage;\n",
    "\n",
    "    double elem = 0;\n",
    "\n",
    "    if (i0 < nx)\n",
    "        elem = data[i0];\n",
    "\n",
    "    // Reduce within the block (all threads *must* participate)\n",
    "    double blockSum = BlockReduce(tempStorage).Sum(elem);\n",
    "\n",
    "    // Atomically add the result to the global sum\n",
    "    if (0 == threadIdx.x && i0 < numElements)\n",
    "        atomicAdd(sum, blockSum);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548414f",
   "metadata": {},
   "source": [
    "Other programming models also support reductions, with varying levels of programming effort, flexibility, and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc12ed0",
   "metadata": {},
   "source": [
    "### OpenMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b379d980",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double sum = 0;\n",
    "\n",
    "#pragma omp target teams distribute parallel for \\\n",
    "            reduction(+ : sum)\n",
    "for (size_t i0 = 0; i0 < nx; ++i0)\n",
    "    sum += data[i0];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb355b2",
   "metadata": {},
   "source": [
    "### OpenACC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f364d552",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double sum = 0;\n",
    "\n",
    "#pragma acc parallel loop present(data[:nx]) \\\n",
    "            reduction(+ : sum)\n",
    "for (size_t i0 = 0; i0 < nx; ++i0)\n",
    "    sum += data[i0];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2284f",
   "metadata": {},
   "source": [
    "### Modern C++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dcd64f",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double sum = std::reduce(std::execution::par_unseq, data, data + nx, 0., std::plus<>{});\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c073ff",
   "metadata": {},
   "source": [
    "### Thrust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce68cad8",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double sum = thrust::reduce(data, data + nx, 0.);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb5169",
   "metadata": {},
   "source": [
    "### Kokkos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739f74b",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double sum = 0;\n",
    "\n",
    "Kokkos::parallel_reduce(\n",
    "    Kokkos::RangePolicy<>(0, nx),\n",
    "    KOKKOS_LAMBDA(const size_t i0, double &acc) {\n",
    "        acc += data(i0);\n",
    "    }, sum);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d62ad87",
   "metadata": {},
   "source": [
    "### SYCL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724469a5",
   "metadata": {},
   "source": [
    "```cpp\n",
    "q.submit([&](sycl::handler &h) {\n",
    "    h.parallel_for(nx, [=](auto i0) {\n",
    "        auto v = sycl::atomic_ref<double, sycl::memory_order::relaxed,\n",
    "                                  sycl::memory_scope::device,\n",
    "                                  sycl::access::address_space::global_space>(\n",
    "            sum[0]);\n",
    "        v.fetch_add(data[i0]);\n",
    "    });\n",
    "});\n",
    "```\n",
    "\n",
    "Additional optimizations, similar to CUDA, are possible.\n",
    "For more information, see [Intel's OneAPI optimization guide](https://www.intel.com/content/www/us/en/docs/oneapi/optimization-guide-gpu/2025-0/reduction.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e40977",
   "metadata": {},
   "source": [
    "### Additional Consideration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52f92de",
   "metadata": {},
   "source": [
    "Often, reductions can be fused with the production of their input values.\n",
    "For example, when computing a dot product, a naive implementation performs two steps:\n",
    "* Compute the point-wise multiplication and store the result in a temporary vector\n",
    "* Apply a sum reduction to the temporary vector\n",
    "\n",
    "Fusing these steps reduces memory usage and typically improves performance by minimizing data movement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b4600",
   "metadata": {},
   "source": [
    "Most of the approaches discussed above support this fusion easily.\n",
    "Two exceptions are modern C++ and Thrust, which require different algorithms:\n",
    "* `std::transform_reduce`\n",
    "* `thrust::transform_reduce` or `thrust::transform_iterator`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f98484f",
   "metadata": {},
   "source": [
    "## Beyond 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59ee5ba-fd5b-4b20-8d54-e79b884ae05a",
   "metadata": {},
   "source": [
    "Many algorithms use multidimensional iteration spaces and data structures, not just 1D.\n",
    "When choosing a GPU programming approach, consider:\n",
    "* Can multidimensional iteration spaces be parallelized intuitively?\n",
    "* Can the thread hierarchy be multidimensional? This can improve data reuse for neighborhood-based access patterns.\n",
    "* Is there support for multidimensional data structures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b9a90-cd5a-4f49-ab35-c3dae9703229",
   "metadata": {},
   "source": [
    "## Interoperability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d885722-2c66-484d-a301-ec90d2ba26b2",
   "metadata": {},
   "source": [
    "Another important consideration is interoperability.\n",
    "Many GPU programming approaches provide interfaces to and from CUDA/HIP on their respective platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1da9f2-96d4-4cd7-b6b9-eb1caad6f03c",
   "metadata": {},
   "source": [
    "## Multi-GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05208088-d364-4f11-ae62-7d908ff38fc2",
   "metadata": {},
   "source": [
    "Scaling workloads to multiple GPUs and even multiple GPU-equipped nodes is crucial in many HPC applications.\n",
    "\n",
    "This requires mechanisms for targeting different GPUs on a node and interoperability with distributed memory solutions such as MPI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff3f5e8",
   "metadata": {},
   "source": [
    "## Next Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729262de",
   "metadata": {},
   "source": [
    "Proceed to the [programming challenge](./programming-challenge.ipynb) notebook to apply what you've learned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
