{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7fa1751",
   "metadata": {},
   "source": [
    "# CUDA/HIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7837bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Level 0: Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a20d50",
   "metadata": {},
   "source": [
    "We start with a serial CPU code printing a range of numbers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e03ebb6",
   "metadata": {},
   "source": [
    "```cpp\n",
    "for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "    printf(\"%ld\\n\", i0);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970a674",
   "metadata": {},
   "source": [
    "The full example is available in [print-numbers-base.cpp](../src/print-numbers/print-numbers-base.cpp), and can be compiled and executed using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1926a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -O3 -march=native -std=c++17 -o ../build/print-numbers/print-numbers-base ../src/print-numbers/print-numbers-base.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e9875",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/print-numbers/print-numbers-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c254ca1e",
   "metadata": {},
   "source": [
    "CUDA/HIP utilize separate *kernel* functions which are *launched* from the host code. By convention, they must return `void` and are marked with the `__global__` keyword.\n",
    "\n",
    "```cpp\n",
    "__global__ void increase(size_t nx) {\n",
    "    for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "        printf(\"%ld\\n\", i0);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The kernel can be configured by providing an *execution configuration* in triple-chevron syntax.\n",
    "\n",
    "```cpp\n",
    "increase<<<1, 1>>>(nx);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f6f217",
   "metadata": {},
   "source": [
    "The above example runs on the GPU, but all work is done in a single thread.\n",
    "To achieve parallelism, you must manually assign each loop iteration to a separate thread and spawn as many threads as there are iterations.\n",
    "Each thread computes a unique global or data index using built-in thread variables.\n",
    "\n",
    "```cpp\n",
    "__global__ void increase(size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    printf(\"%ld\\n\", i0);\n",
    "}\n",
    "```\n",
    "\n",
    "```cpp\n",
    "auto numThreadsPerBlock = 256;\n",
    "auto numBlocks = nx / numThreadsPerBlock;\n",
    "increase<<<numBlocks, numThreadsPerBlock>>>(nx);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af94347",
   "metadata": {},
   "source": [
    "While the above example works if `nx` is evenly divisible by the block size, it will not in all other cases.\n",
    "The common solution is to spawn an extra block and ensure that only valid threads perform computations.\n",
    "\n",
    "```cpp\n",
    "__global__ void increase(size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (i0 < nx)\n",
    "        printf(\"%ld\\n\", i0);\n",
    "}\n",
    "```\n",
    "\n",
    "```cpp\n",
    "auto numThreadsPerBlock = 256;\n",
    "auto numBlocks = ceilingDivide(nx, numThreadsPerBlock);\n",
    "increase<<<numBlocks, numThreadsPerBlock>>>(nx);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418cfb36",
   "metadata": {},
   "source": [
    "The complete example code is available in [print-numbers-cuda.cpp](../src/print-numbers/print-numbers-cuda.cpp), and can be built and executed using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6137f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -std=c++17 -arch=sm_86 -o ../build/print-numbers/print-numbers-cuda ../src/print-numbers/print-numbers-cuda.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5104a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/print-numbers/print-numbers-cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af6a43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Level 1: Adding Managed Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6b0a6e",
   "metadata": {},
   "source": [
    "Our next application is increasing all elements of an array by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43501576",
   "metadata": {},
   "source": [
    "[increase-base.cpp](../src/increase/increase-base.cpp) shows a serial CPU-only implementation.\n",
    "Its key part and our entry point is the increase function.\n",
    "\n",
    "```cpp\n",
    "void increase(double* data, size_t nx) {\n",
    "    for (size_t i0 = 0; i0 < nx; ++i0) {\n",
    "        data[i0] += 1;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc82dc",
   "metadata": {},
   "source": [
    "Data allocation and deallocation is done explicitly in CUDA.\n",
    "For the case of managed memory this works as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d81db6a",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double *data;                  // unified allocation\n",
    "cudaMallocManaged((void **)&data, sizeof(double) * nx);\n",
    "\n",
    "/* ... */\n",
    "\n",
    "cudaFree(data);                // unified de-allocation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f84f2",
   "metadata": {},
   "source": [
    "Migration of data structures is implicit, but can be triggered explicitly to optimize performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471be7fc",
   "metadata": {},
   "source": [
    "```cpp\n",
    "cudaMemPrefetchAsync(data, sizeof(double) * nx, 0 /* deviceId */);   // host to device\n",
    "cudaMemPrefetchAsync(data, sizeof(double) * nx, cudaCpuDeviceId);    // device to host\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf48dde",
   "metadata": {},
   "source": [
    "Pointers to allocated memory are passed to the kernel as argument.\n",
    "\n",
    "```cpp\n",
    "__global__ void increase(double* data, size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (i0 < nx)\n",
    "        data[i0] += 1;\n",
    "}\n",
    "```\n",
    "\n",
    "```cpp\n",
    "auto numThreadsPerBlock = 256;\n",
    "auto numBlocks = ceilingDivide(nx, numThreadsPerBlock);\n",
    "increase<<<numBlocks, numThreadsPerBlock>>>(data, nx);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9e181",
   "metadata": {},
   "source": [
    "The complete example code is available in [increase-cuda-mm.cpp](../src/increase/increase-cuda-mm.cpp).\n",
    "Build and execute it using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -std=c++17 -arch=sm_86 -o ../build/increase/increase-cuda-mm ../src/increase/increase-cuda-mm.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555fd818",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-cuda-mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff69508",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Level 2: Switching to Explicit Memory Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be75930b",
   "metadata": {},
   "source": [
    "Switching from managed memory to explicit memory management requires the following changes and additions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3823c02e",
   "metadata": {},
   "source": [
    "**1.** Separate device and host allocations\n",
    "\n",
    "A frequently used pattern is having two separate pointers for host and device allocations, which share the same *data layout*.\n",
    "To make this intend clear to code readers, often the same variable name is used but prefixed with a `d_` for the device version.\n",
    "An additional prefixing with `h_` for host versions can be even more verbose, but is less often used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4081409",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double *data;                  // host allocation\n",
    "cudaMallocHost((void **)&data, sizeof(double) * nx);      // HIP uses hipHostMalloc\n",
    "\n",
    "double *d_data;                // device allocation    \n",
    "cudaMalloc((void **)&d_data, sizeof(double) * nx);\n",
    "\n",
    "/* ... */\n",
    "\n",
    "cudaFree(d_data);              // device de-allocation\n",
    "\n",
    "cudaFreeHost(data);            // host de-allocation   // HIP uses hipHostFree\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e0977",
   "metadata": {},
   "source": [
    "**2.** Explicit copies between host and device\n",
    "\n",
    "Data transfer is performed with `cudaMemcpy(target, source, bytesToTransfer, direction)`, for example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42542f5",
   "metadata": {},
   "source": [
    "```cpp\n",
    "cudaMemcpy(d_data, data, sizeof(double) * nx, cudaMemcpyHostToDevice);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb3799",
   "metadata": {},
   "source": [
    "As before, pointers to (device) memory can be passed to kernels as argument.\n",
    "Note, that the `d_` prefix is commonly dropped inside the kernel.\n",
    "\n",
    "```cpp\n",
    "__global__ void increase(double* data, size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (i0 < nx)\n",
    "        data[i0] += 1;\n",
    "}\n",
    "```\n",
    "\n",
    "```cpp\n",
    "auto numThreadsPerBlock = 256;\n",
    "auto numBlocks = ceilingDivide(nx, numThreadsPerBlock);\n",
    "increase<<<numBlocks, numThreadsPerBlock>>>(d_data, nx);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a24301",
   "metadata": {},
   "source": [
    "The complete example code is available in [increase-cuda-expl.cpp](../src/increase/increase-cuda-expl.cpp).\n",
    "Build and execute it using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65958ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -std=c++17 -arch=sm_86 -o ../build/increase/increase-cuda-expl ../src/increase/increase-cuda-expl.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b255d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/increase/increase-cuda-expl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
